{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4175b8f9-e1f7-4686-a6cd-ec32968ec291",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\isaac\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Keras\n",
    "import tensorflow as tf\n",
    "# Non-Keras Loads\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "#Cleaning Loads\n",
    "import regex as re\n",
    "import emoji\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "import requests\n",
    "from collections import Counter\n",
    "\n",
    "#Visualization\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c0b838f-5b67-4efa-8117-a95895745ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "                                                text  target\n",
      "0  Our Deeds are the Reason of this #earthquake M...       1\n",
      "1             Forest fire near La Ronge Sask. Canada       1\n",
      "2  All residents asked to 'shelter in place' are ...       1\n",
      "3  13,000 people receive #wildfires evacuation or...       1\n",
      "4  Just got sent this photo from Ruby #Alaska as ...       1\n",
      "test\n",
      "   id                                               text\n",
      "0   0                 Just happened a terrible car crash\n",
      "1   2  Heard about #earthquake is different cities, s...\n",
      "2   3  there is a forest fire at spot pond, geese are...\n",
      "3   9           Apocalypse lighting. #Spokane #wildfires\n",
      "4  11      Typhoon Soudelor kills 28 in China and Taiwan\n"
     ]
    }
   ],
   "source": [
    "train_path = \"nlp-getting-started/train.csv\"\n",
    "test_path = \"nlp-getting-started/test.csv\"\n",
    "train = pd.read_csv(train_path, header = 0)\n",
    "train = train.drop(labels = [\"keyword\", \"location\", \"id\"], axis = 1)\n",
    "test = pd.read_csv(test_path, header = 0)\n",
    "test = test.drop(labels = [\"keyword\", \"location\"], axis = 1)\n",
    "print(\"train\")\n",
    "print(train.head())\n",
    "print(\"test\")\n",
    "print(test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51beb3b9-5ac4-4dc0-8771-591e8e120a40",
   "metadata": {},
   "source": [
    "# EDA\n",
    "The dataset has 57% non-disaster tweets, and 43% Disaster tweets.  There are 31924 unique words.  This will drive my tuning the vectorization of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d40f1d1-cb36-4217-b481-1c7bbaa14c2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    7613.00000\n",
       "mean        0.42966\n",
       "std         0.49506\n",
       "min         0.00000\n",
       "25%         0.00000\n",
       "50%         0.00000\n",
       "75%         1.00000\n",
       "max         1.00000\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['target'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5e9a86d-404a-424a-9212-840ed54f431f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31924\n"
     ]
    }
   ],
   "source": [
    "unique_words = set()\n",
    "for text in train[\"text\"]:\n",
    "    unique_words.update(text.split())\n",
    "print(len(unique_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144bb3c7-be41-496f-ac0a-8a901627e5be",
   "metadata": {},
   "source": [
    "Cleaning\n",
    "\n",
    "Standard tweet cleaning. Cleaning found at: https://stackoverflow.com/questions/64719706/cleaning-twitter-data-pandas-python\n",
    "Tokenizatin\n",
    "\n",
    "I tolkenized the tweets in preparation to convert to tensors for embedding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22560826-683e-45fe-a909-6c48827f7919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thank you Chatgpt for this\n",
    "def download_file_from_github(url):\n",
    "    \"\"\"Download a file from a GitHub URL and return its contents as a list of lines.\"\"\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        lines = response.text.splitlines()  # Split the content into lines\n",
    "        return lines  # You could change this to `set(lines)` if you need a set instead of a list\n",
    "    else:\n",
    "        raise Exception(f\"Failed to download file: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b06625b-b30c-432c-b9e6-0202c93aa2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame()\n",
    "stop_words = download_file_from_github(\"https://raw.githubusercontent.com/stopwords-iso/stopwords-en/master/raw/stop-words-english1.txt\")\n",
    "contractions = download_file_from_github(\"https://gist.githubusercontent.com/J3RN/ed7b420a6ea1d5bd6d06/raw/acda66b325a2b4d7282fb602a7551912cdc81e74/contractions.txt\")\n",
    "def cleaning(line):\n",
    "    tweet = line['text']\n",
    "    tweet = tweet.lower()\n",
    "    tweet = re.sub(\"@[A-Za-z0-9]+\",\"\",tweet) #Remove @ sign\n",
    "    tweet = re.sub(r\"(?:\\@|http?\\://|https?\\://|www)\\S+\", \"\", tweet) #Remove http links\n",
    "    tweet = \" \".join(tweet.split())\n",
    "    tweet = emoji.replace_emoji(tweet, '') #Remove Emojis\n",
    "    tweet = tweet.replace(\"#\", \"\").replace(\"_\", \" \") #Remove hashtag sign but keep the text\n",
    "    tweet = re.sub(r'[^a-z]', ' ', tweet) # Strip all symbols and replace with +\n",
    "    tweet = re.sub(r'\\b\\w{1,2}\\b+', '', tweet) #get rid of all words <= 2 characters\n",
    "    #Tolkenize the Text\n",
    "    word_tokens = word_tokenize(tweet)\n",
    "    word_tokens = [w for w in word_tokens if not w in stop_words]\n",
    "    word_tokens = [w for w in word_tokens if not w in contractions]\n",
    "    \n",
    "    #tknzr = nltk.tokenize.casual.TweetTokenizer(preserve_case = False, strip_handles = True, reduce_len = True)\n",
    "    #tweet = tknzr.tokenize(tweet)\n",
    "    return word_tokens \n",
    "\n",
    "train['cleaned_text'] = train.apply(cleaning, axis = 1)\n",
    "test['cleaned_text'] = test.apply(cleaning, axis = 1)\n",
    "\n",
    "\n",
    "# Return text to string for processing later\n",
    "train['cleaned_text'] = train['cleaned_text'].apply(\" \".join)\n",
    "test['cleaned_text'] = test['cleaned_text'].apply(\" \".join)\n",
    "train['target'] = train['target'].astype('float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c340bd27-c6f5-4176-b60f-ed38427122bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 text  target  \\\n",
      "0   Our Deeds are the Reason of this #earthquake M...     1.0   \n",
      "1              Forest fire near La Ronge Sask. Canada     1.0   \n",
      "2   All residents asked to 'shelter in place' are ...     1.0   \n",
      "3   13,000 people receive #wildfires evacuation or...     1.0   \n",
      "4   Just got sent this photo from Ruby #Alaska as ...     1.0   \n",
      "5   #RockyFire Update => California Hwy. 20 closed...     1.0   \n",
      "6   #flood #disaster Heavy rain causes flash flood...     1.0   \n",
      "7   I'm on top of the hill and I can see a fire in...     1.0   \n",
      "8   There's an emergency evacuation happening now ...     1.0   \n",
      "9   I'm afraid that the tornado is coming to our a...     1.0   \n",
      "10        Three people died from the heat wave so far     1.0   \n",
      "11  Haha South Tampa is getting flooded hah- WAIT ...     1.0   \n",
      "12  #raining #flooding #Florida #TampaBay #Tampa 1...     1.0   \n",
      "13            #Flood in Bago Myanmar #We arrived Bago     1.0   \n",
      "14  Damage to school bus on 80 in multi car crash ...     1.0   \n",
      "15                                     What's up man?     0.0   \n",
      "16                                      I love fruits     0.0   \n",
      "17                                   Summer is lovely     0.0   \n",
      "18                                  My car is so fast     0.0   \n",
      "19                       What a goooooooaaaaaal!!!!!!     0.0   \n",
      "\n",
      "                                         cleaned_text  \n",
      "0               deeds reason earthquake allah forgive  \n",
      "1                       forest fire ronge sask canada  \n",
      "2   residents asked shelter place notified officer...  \n",
      "3   people receive wildfires evacuation orders cal...  \n",
      "4      photo ruby alaska smoke wildfires pours school  \n",
      "5   rockyfire update california hwy closed directi...  \n",
      "6   flood disaster heavy rain flash flooding stree...  \n",
      "7                                 top hill fire woods  \n",
      "8      emergency evacuation happening building street  \n",
      "9                          afraid tornado coming area  \n",
      "10                              people died heat wave  \n",
      "11  haha south tampa flooded hah wait live south t...  \n",
      "12  raining flooding florida tampabay tampa days l...  \n",
      "13                    flood bago myanmar arrived bago  \n",
      "14         damage school bus multi car crash breaking  \n",
      "15                                                man  \n",
      "16                                        love fruits  \n",
      "17                                      summer lovely  \n",
      "18                                           car fast  \n",
      "19                                    goooooooaaaaaal  \n",
      "Rows processed with 0 remaining words\n",
      "                                                   text  target cleaned_text\n",
      "30                                             The end!     0.0             \n",
      "428             who makes these? http://t.co/28t3NWHdKy     0.0             \n",
      "1664                                 @Collapsed thank u     0.0             \n",
      "3584   @_AsianShawtyy ?????????? I'm sorry. But I'm out     0.0             \n",
      "3680                         That usually NEVER happens     0.0             \n",
      "4297  @HellFire_eV @JackPERU1 then I do this to one ...     0.0             \n",
      "4891                        @eileenmfl are you serious?     0.0             \n",
      "5564                          @NathanFillion Hardly! ??     0.0             \n"
     ]
    }
   ],
   "source": [
    "print(train.head(20))\n",
    "zero_length_lists = train[train['cleaned_text'].apply(lambda x: len(x) == 0)]\n",
    "print(\"Rows processed with 0 remaining words\")\n",
    "print(zero_length_lists)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f39157-7b29-4dd7-8141-e0c47d25ca2b",
   "metadata": {},
   "source": [
    "# Additional EDA.  \n",
    "Thats intresting.  All the items that are all stopwords are going to be no an emergency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09b96d91-7fc7-4b72-8a95-39c8ffe6dd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets\n",
    "train_dat = train[['cleaned_text','target']].sample(frac = .9)\n",
    "val_dat = train[['cleaned_text','target']].drop(train_dat.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8d2e21-4a02-4eac-8734-655aa3fe1cba",
   "metadata": {},
   "source": [
    "Vectorizing\n",
    "\n",
    "I used Keras internal vectorizer to vectorize the text. I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7afd6bf9-dbf2-4f61-9b46-362fe1409a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "train_tf_data = tf.data.Dataset.from_tensor_slices(\n",
    "    (train_dat['cleaned_text'].values, \n",
    "     train_dat['target'].values)).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "val_tf_data = tf.data.Dataset.from_tensor_slices(\n",
    "    (val_dat['cleaned_text'].values, \n",
    "     val_dat['target'].values)).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bcaa7127-390c-4d7c-b398-3ffca8d34850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['' '[UNK]' 'amp' 'fire' 'news' 'don' 'people' 'video' 'disaster'\n",
      " 'emergency' 'police' 'time' 'body' 'suicide' 'storm' 'california'\n",
      " 'burning' 'crash' 'day' 'man']\n"
     ]
    }
   ],
   "source": [
    "# https://www.tensorflow.org/text/tutorials/text_classification_rnn\n",
    "\n",
    "VOCAB_SIZE = 1000\n",
    "encoder = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE)\n",
    "encoder.adapt(train_tf_data.map(lambda text, label: text))\n",
    "vocab = np.array(encoder.get_vocabulary())\n",
    "print(vocab[:20])\n",
    "#encoded_example = encoder(train_tf_data)[:3].numpy()\n",
    "#print(encoded_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b704695-3c72-48f7-8455-ff3578cd24c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    # Plot training & validation accuracy values\n",
    "    plt.plot(history.history['binary_accuracy'])\n",
    "    plt.plot(history.history['val_binary_accuracy'])\n",
    "    plt.title('Model accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot training & validation loss values\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5dfb92e5-e1e2-465a-bc51-d2ab00db8c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/text/tutorials/text_classification_rnn\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    encoder,\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=len(encoder.get_vocabulary()),\n",
    "        output_dim=64,\n",
    "        # Use masking to handle the variable sequence lengths\n",
    "        mask_zero=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a50b767-275a-49cf-9da2-8bc1d7c42e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              metrics=[\"binary_accuracy\", \"F1Score\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d722c50e-f405-40a6-a9ef-cd960ec0a535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "213/215 [============================>.] - ETA: 0s - loss: 0.1532 - binary_accuracy: 0.9410 - f1_score: 0.6003WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 30 batches). You may need to use the repeat() function when building your dataset.\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.1530 - binary_accuracy: 0.9412 - f1_score: 0.6001 - val_loss: 1.8585 - val_binary_accuracy: 0.7293 - val_f1_score: 0.6100\n",
      "Epoch 2/10\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.1502 - binary_accuracy: 0.9428 - f1_score: 0.6001\n",
      "Epoch 3/10\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.1483 - binary_accuracy: 0.9428 - f1_score: 0.6001\n",
      "Epoch 4/10\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.1477 - binary_accuracy: 0.9432 - f1_score: 0.6001\n",
      "Epoch 5/10\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.1450 - binary_accuracy: 0.9437 - f1_score: 0.6001\n",
      "Epoch 6/10\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.1445 - binary_accuracy: 0.9438 - f1_score: 0.6001\n",
      "Epoch 7/10\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.1433 - binary_accuracy: 0.9437 - f1_score: 0.6001\n",
      "Epoch 8/10\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.1484 - binary_accuracy: 0.9424 - f1_score: 0.6001\n",
      "Epoch 9/10\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.1430 - binary_accuracy: 0.9422 - f1_score: 0.6001\n",
      "Epoch 10/10\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.1413 - binary_accuracy: 0.9431 - f1_score: 0.6001\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 2.1615 - binary_accuracy: 0.7162 - f1_score: 0.6100\n",
      "[2.1614630222320557, 0.716162919998169, array([0.6100457], dtype=float32)]\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 2.1615 - binary_accuracy: 0.7162 - f1_score: 0.6100\n",
      "Test Loss:  2.1614630222320557\n",
      "Test Accuracy:  0.716162919998169\n",
      "Val F1:  0.6100457\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_tf_data, epochs=10,\n",
    "                    validation_data=val_tf_data,\n",
    "                    validation_steps=1)\n",
    "print(model.evaluate(val_tf_data))\n",
    "test_loss, test_acc, val_f1 = model.evaluate(val_tf_data)\n",
    "print('Test Loss: ', test_loss)\n",
    "print('Test Accuracy: ', test_acc)\n",
    "print('Val F1: ', val_f1[0])\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "adc24727-8822-43e6-8c5d-ce65e35ef670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "214/215 [============================>.] - ETA: 0s - loss: 0.6838 - binary_accuracy: 0.5711 - f1_score: 0.6003WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 30 batches). You may need to use the repeat() function when building your dataset.\n",
      "215/215 [==============================] - 20s 52ms/step - loss: 0.6837 - binary_accuracy: 0.5714 - f1_score: 0.6001 - val_loss: 0.6696 - val_binary_accuracy: 0.5611 - val_f1_score: 0.6100\n",
      "Epoch 2/10\n",
      "215/215 [==============================] - 7s 35ms/step - loss: 0.5853 - binary_accuracy: 0.6783 - f1_score: 0.6001\n",
      "Epoch 3/10\n",
      "215/215 [==============================] - 7s 35ms/step - loss: 0.4709 - binary_accuracy: 0.7858 - f1_score: 0.6001\n",
      "Epoch 4/10\n",
      "215/215 [==============================] - 7s 35ms/step - loss: 0.4328 - binary_accuracy: 0.8058 - f1_score: 0.6001\n",
      "Epoch 5/10\n",
      "215/215 [==============================] - 7s 35ms/step - loss: 0.4119 - binary_accuracy: 0.8189 - f1_score: 0.6001\n",
      "Epoch 6/10\n",
      "215/215 [==============================] - 8s 35ms/step - loss: 0.3973 - binary_accuracy: 0.8273 - f1_score: 0.6001\n",
      "Epoch 7/10\n",
      "215/215 [==============================] - 7s 35ms/step - loss: 0.3861 - binary_accuracy: 0.8361 - f1_score: 0.6001\n",
      "Epoch 8/10\n",
      "215/215 [==============================] - 7s 35ms/step - loss: 0.3770 - binary_accuracy: 0.8398 - f1_score: 0.6001\n",
      "Epoch 9/10\n",
      "215/215 [==============================] - 7s 35ms/step - loss: 0.3690 - binary_accuracy: 0.8425 - f1_score: 0.6001\n",
      "Epoch 10/10\n",
      "215/215 [==============================] - 7s 35ms/step - loss: 0.3618 - binary_accuracy: 0.8452 - f1_score: 0.6001\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.5376 - binary_accuracy: 0.7569 - f1_score: 0.6100\n",
      "[0.5375680923461914, 0.756898820400238, array([0.6100457], dtype=float32)]\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.5376 - binary_accuracy: 0.7569 - f1_score: 0.6100\n",
      "Test Loss:  0.5375680923461914\n",
      "Test Accuracy:  0.756898820400238\n",
      "Val F1:  0.6100457\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    encoder,\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=len(encoder.get_vocabulary()),\n",
    "        output_dim=64,\n",
    "        # Use masking to handle the variable sequence lengths\n",
    "        mask_zero=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              metrics=[\"binary_accuracy\", \"F1Score\"])\n",
    "history = model.fit(train_tf_data, epochs=10,\n",
    "                    validation_data=val_tf_data,\n",
    "                    validation_steps=1)\n",
    "print(model.evaluate(val_tf_data))\n",
    "test_loss, test_acc, val_f1 = model.evaluate(val_tf_data)\n",
    "print('Test Loss: ', test_loss)\n",
    "print('Test Accuracy: ', test_acc)\n",
    "print('Val F1: ', val_f1[0])\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8a62926a-c431-4701-bd68-030031eb1562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRU Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "75f7fe35-e768-43a3-bb8b-d6d96ca2b06c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "214/215 [============================>.] - ETA: 0s - loss: 0.6798 - binary_accuracy: 0.5711 - f1_score: 0.6003WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 30 batches). You may need to use the repeat() function when building your dataset.\n",
      "215/215 [==============================] - 23s 56ms/step - loss: 0.6797 - binary_accuracy: 0.5714 - f1_score: 0.6001 - val_loss: 0.6673 - val_binary_accuracy: 0.5611 - val_f1_score: 0.6100\n",
      "Epoch 2/10\n",
      "215/215 [==============================] - 9s 40ms/step - loss: 0.5841 - binary_accuracy: 0.6639 - f1_score: 0.6001\n",
      "Epoch 3/10\n",
      "215/215 [==============================] - 9s 40ms/step - loss: 0.4382 - binary_accuracy: 0.7983 - f1_score: 0.6001\n",
      "Epoch 4/10\n",
      "215/215 [==============================] - 9s 40ms/step - loss: 0.4046 - binary_accuracy: 0.8176 - f1_score: 0.6001\n",
      "Epoch 5/10\n",
      "215/215 [==============================] - 9s 40ms/step - loss: 0.3884 - binary_accuracy: 0.8262 - f1_score: 0.6001\n",
      "Epoch 6/10\n",
      "215/215 [==============================] - 9s 40ms/step - loss: 0.3776 - binary_accuracy: 0.8335 - f1_score: 0.6001\n",
      "Epoch 7/10\n",
      "215/215 [==============================] - 9s 40ms/step - loss: 0.3692 - binary_accuracy: 0.8381 - f1_score: 0.6001\n",
      "Epoch 8/10\n",
      "215/215 [==============================] - 9s 40ms/step - loss: 0.3622 - binary_accuracy: 0.8398 - f1_score: 0.6001\n",
      "Epoch 9/10\n",
      "215/215 [==============================] - 9s 40ms/step - loss: 0.3559 - binary_accuracy: 0.8409 - f1_score: 0.6001\n",
      "Epoch 10/10\n",
      "215/215 [==============================] - 9s 40ms/step - loss: 0.3499 - binary_accuracy: 0.8453 - f1_score: 0.6001\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.5156 - binary_accuracy: 0.7714 - f1_score: 0.6100\n",
      "[0.5156311988830566, 0.7713534832000732, array([0.6100457], dtype=float32)]\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.5156 - binary_accuracy: 0.7714 - f1_score: 0.6100\n",
      "Test Loss:  0.5156311988830566\n",
      "Test Accuracy:  0.7713534832000732\n",
      "Val F1:  0.6100457\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    encoder,\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=len(encoder.get_vocabulary()),\n",
    "        output_dim=64,\n",
    "        # Use masking to handle the variable sequence lengths\n",
    "        mask_zero=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.GRU(64, return_sequences=True)),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.GRU(32)),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              metrics=[\"binary_accuracy\", \"F1Score\"])\n",
    "history = model.fit(train_tf_data, epochs=10,\n",
    "                    validation_data=val_tf_data,\n",
    "                    validation_steps=1)\n",
    "print(model.evaluate(val_tf_data))\n",
    "test_loss, test_acc, val_f1 = model.evaluate(val_tf_data)\n",
    "print('Test Loss: ', test_loss)\n",
    "print('Test Accuracy: ', test_acc)\n",
    "print('Val F1: ', val_f1[0])\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d57d25-90a6-4c4a-987b-d9a636ef3887",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cfae489c-bf1a-4291-8a9f-db0e737a6578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increasing the embedding Dim Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c2409e1b-396d-4169-8bff-3855f2f95f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "215/215 [==============================] - ETA: 0s - loss: 0.6618 - binary_accuracy: 0.5950 - f1_score: 0.6001WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 30 batches). You may need to use the repeat() function when building your dataset.\n",
      "215/215 [==============================] - 65s 212ms/step - loss: 0.6618 - binary_accuracy: 0.5950 - f1_score: 0.6001 - val_loss: 0.5906 - val_binary_accuracy: 0.6728 - val_f1_score: 0.6100\n",
      "Epoch 2/10\n",
      "215/215 [==============================] - 32s 151ms/step - loss: 0.4818 - binary_accuracy: 0.7860 - f1_score: 0.6001\n",
      "Epoch 3/10\n",
      "215/215 [==============================] - 32s 151ms/step - loss: 0.4199 - binary_accuracy: 0.8214 - f1_score: 0.6001\n",
      "Epoch 4/10\n",
      "215/215 [==============================] - 33s 153ms/step - loss: 0.3978 - binary_accuracy: 0.8332 - f1_score: 0.6001\n",
      "Epoch 5/10\n",
      "215/215 [==============================] - 32s 151ms/step - loss: 0.3838 - binary_accuracy: 0.8427 - f1_score: 0.6001\n",
      "Epoch 6/10\n",
      "215/215 [==============================] - 33s 151ms/step - loss: 0.3726 - binary_accuracy: 0.8511 - f1_score: 0.6001\n",
      "Epoch 7/10\n",
      "215/215 [==============================] - 33s 155ms/step - loss: 0.3631 - binary_accuracy: 0.8561 - f1_score: 0.6001\n",
      "Epoch 8/10\n",
      "215/215 [==============================] - 33s 155ms/step - loss: 0.3536 - binary_accuracy: 0.8615 - f1_score: 0.6001\n",
      "Epoch 9/10\n",
      "215/215 [==============================] - 33s 151ms/step - loss: 0.3436 - binary_accuracy: 0.8669 - f1_score: 0.6001\n",
      "Epoch 10/10\n",
      "215/215 [==============================] - 32s 151ms/step - loss: 0.3334 - binary_accuracy: 0.8751 - f1_score: 0.6001\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 0.5517 - binary_accuracy: 0.7740 - f1_score: 0.6100\n",
      "[0.5516563057899475, 0.7739816308021545, array([0.6100457], dtype=float32)]\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 0.5517 - binary_accuracy: 0.7740 - f1_score: 0.6100\n",
      "Test Loss:  0.5516563057899475\n",
      "Test Accuracy:  0.7739816308021545\n",
      "Val F1:  0.6100457\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    encoder,\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=len(encoder.get_vocabulary()),\n",
    "        output_dim=128,\n",
    "        # Use masking to handle the variable sequence lengths\n",
    "        mask_zero=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.GRU(128, return_sequences=True)),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.GRU(64, return_sequences=True)),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.GRU(32, return_sequences=True)),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.GRU(16)),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              metrics=[\"binary_accuracy\", \"F1Score\"])\n",
    "history = model.fit(train_tf_data, epochs=10,\n",
    "                    validation_data=val_tf_data,\n",
    "                    validation_steps=1)\n",
    "test_loss, test_acc, val_f1 = model.evaluate(val_tf_data)\n",
    "print('Test Loss: ', test_loss)\n",
    "print('Test Accuracy: ', test_acc)\n",
    "print('Val F1: ', val_f1[0])\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6aea177a-683f-4166-9a5a-bb8b24a415b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "215/215 [==============================] - ETA: 0s - loss: 0.6518 - binary_accuracy: 0.6068 - f1_score: 0.6001WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 30 batches). You may need to use the repeat() function when building your dataset.\n",
      "215/215 [==============================] - 65s 207ms/step - loss: 0.6518 - binary_accuracy: 0.6068 - f1_score: 0.6001 - val_loss: 0.5738 - val_binary_accuracy: 0.7057 - val_f1_score: 0.6100\n",
      "Epoch 2/10\n",
      "215/215 [==============================] - 37s 172ms/step - loss: 0.4676 - binary_accuracy: 0.7855 - f1_score: 0.6001\n",
      "Epoch 3/10\n",
      "215/215 [==============================] - 37s 174ms/step - loss: 0.4135 - binary_accuracy: 0.8165 - f1_score: 0.6001\n",
      "Epoch 4/10\n",
      "215/215 [==============================] - 37s 173ms/step - loss: 0.3931 - binary_accuracy: 0.8294 - f1_score: 0.6001\n",
      "Epoch 5/10\n",
      "215/215 [==============================] - 37s 172ms/step - loss: 0.3799 - binary_accuracy: 0.8368 - f1_score: 0.6001\n",
      "Epoch 6/10\n",
      "215/215 [==============================] - 37s 173ms/step - loss: 0.3695 - binary_accuracy: 0.8431 - f1_score: 0.6001\n",
      "Epoch 7/10\n",
      "215/215 [==============================] - 37s 173ms/step - loss: 0.3601 - binary_accuracy: 0.8476 - f1_score: 0.6001\n",
      "Epoch 8/10\n",
      "215/215 [==============================] - 37s 174ms/step - loss: 0.3505 - binary_accuracy: 0.8535 - f1_score: 0.6001\n",
      "Epoch 9/10\n",
      "215/215 [==============================] - 38s 177ms/step - loss: 0.3399 - binary_accuracy: 0.8625 - f1_score: 0.6001\n",
      "Epoch 10/10\n",
      "215/215 [==============================] - 37s 174ms/step - loss: 0.3283 - binary_accuracy: 0.8701 - f1_score: 0.6001\n",
      "24/24 [==============================] - 1s 45ms/step - loss: 0.5656 - binary_accuracy: 0.7727 - f1_score: 0.6100\n",
      "Test Loss:  0.5655559301376343\n",
      "Test Accuracy:  0.7726675271987915\n",
      "Val F1:  0.6100457\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    encoder,\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=len(encoder.get_vocabulary()),\n",
    "        output_dim=128,\n",
    "        # Use masking to handle the variable sequence lengths\n",
    "        mask_zero=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.GRU(128, return_sequences=True)),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.GRU(64, return_sequences=True)),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.GRU(32, return_sequences=True)),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.GRU(16)),\n",
    "    tf.keras.layers.Dense(16, activation='tanh'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              metrics=[\"binary_accuracy\", \"F1Score\"])\n",
    "history = model.fit(train_tf_data, epochs=10,\n",
    "                    validation_data=val_tf_data,\n",
    "                    validation_steps=1)\n",
    "test_loss, test_acc, val_f1 = model.evaluate(val_tf_data)\n",
    "print('Test Loss: ', test_loss)\n",
    "print('Test Accuracy: ', test_acc)\n",
    "print('Val F1: ', val_f1[0])\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fd3375-1702-4710-8ad5-1097ba3274bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
