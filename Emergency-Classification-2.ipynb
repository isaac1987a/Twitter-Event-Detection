{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4175b8f9-e1f7-4686-a6cd-ec32968ec291",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\isaac\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Keras\n",
    "import tensorflow as tf\n",
    "# Non-Keras Loads\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "#Cleaning Loads\n",
    "import regex as re\n",
    "import emoji\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "import requests\n",
    "from collections import Counter\n",
    "\n",
    "#Visualization\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c0b838f-5b67-4efa-8117-a95895745ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "                                                text  target\n",
      "0  Our Deeds are the Reason of this #earthquake M...       1\n",
      "1             Forest fire near La Ronge Sask. Canada       1\n",
      "2  All residents asked to 'shelter in place' are ...       1\n",
      "3  13,000 people receive #wildfires evacuation or...       1\n",
      "4  Just got sent this photo from Ruby #Alaska as ...       1\n",
      "test\n",
      "   id                                               text\n",
      "0   0                 Just happened a terrible car crash\n",
      "1   2  Heard about #earthquake is different cities, s...\n",
      "2   3  there is a forest fire at spot pond, geese are...\n",
      "3   9           Apocalypse lighting. #Spokane #wildfires\n",
      "4  11      Typhoon Soudelor kills 28 in China and Taiwan\n"
     ]
    }
   ],
   "source": [
    "train_path = \"nlp-getting-started/train.csv\"\n",
    "test_path = \"nlp-getting-started/test.csv\"\n",
    "train = pd.read_csv(train_path, header = 0)\n",
    "train = train.drop(labels = [\"keyword\", \"location\", \"id\"], axis = 1)\n",
    "test = pd.read_csv(test_path, header = 0)\n",
    "test = test.drop(labels = [\"keyword\", \"location\"], axis = 1)\n",
    "print(\"train\")\n",
    "print(train.head())\n",
    "print(\"test\")\n",
    "print(test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51beb3b9-5ac4-4dc0-8771-591e8e120a40",
   "metadata": {},
   "source": [
    "# EDA\n",
    "The dataset has 57% non-disaster tweets, and 43% Disaster tweets.  There are 31924 unique words.  This will drive my tuning the vectorization of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d40f1d1-cb36-4217-b481-1c7bbaa14c2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    7613.00000\n",
       "mean        0.42966\n",
       "std         0.49506\n",
       "min         0.00000\n",
       "25%         0.00000\n",
       "50%         0.00000\n",
       "75%         1.00000\n",
       "max         1.00000\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['target'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5e9a86d-404a-424a-9212-840ed54f431f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31924\n"
     ]
    }
   ],
   "source": [
    "unique_words = set()\n",
    "for text in train[\"text\"]:\n",
    "    unique_words.update(text.split())\n",
    "print(len(unique_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144bb3c7-be41-496f-ac0a-8a901627e5be",
   "metadata": {},
   "source": [
    "Cleaning\n",
    "\n",
    "Standard tweet cleaning. Cleaning found at: https://stackoverflow.com/questions/64719706/cleaning-twitter-data-pandas-python\n",
    "Tokenizatin\n",
    "\n",
    "I tolkenized the tweets in preparation to convert to tensors for embedding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22560826-683e-45fe-a909-6c48827f7919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thank you Chatgpt for this\n",
    "def download_file_from_github(url):\n",
    "    \"\"\"Download a file from a GitHub URL and return its contents as a list of lines.\"\"\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        lines = response.text.splitlines()  # Split the content into lines\n",
    "        return lines  # You could change this to `set(lines)` if you need a set instead of a list\n",
    "    else:\n",
    "        raise Exception(f\"Failed to download file: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8b06625b-b30c-432c-b9e6-0202c93aa2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame()\n",
    "stop_words = download_file_from_github(\"https://raw.githubusercontent.com/stopwords-iso/stopwords-en/master/raw/stop-words-english1.txt\")\n",
    "contractions = download_file_from_github(\"https://gist.githubusercontent.com/J3RN/ed7b420a6ea1d5bd6d06/raw/acda66b325a2b4d7282fb602a7551912cdc81e74/contractions.txt\")\n",
    "def cleaning(line):\n",
    "    tweet = line['text']\n",
    "    tweet = tweet.lower()\n",
    "    tweet = re.sub(\"@[A-Za-z0-9]+\",\"\",tweet) #Remove @ sign\n",
    "    tweet = re.sub(r\"(?:\\@|http?\\://|https?\\://|www)\\S+\", \"\", tweet) #Remove http links\n",
    "    tweet = \" \".join(tweet.split())\n",
    "    tweet = emoji.replace_emoji(tweet, '') #Remove Emojis\n",
    "    tweet = tweet.replace(\"#\", \"\").replace(\"_\", \" \") #Remove hashtag sign but keep the text\n",
    "    tweet = re.sub(r'[^a-z]', ' ', tweet) # Strip all symbols and replace with +\n",
    "    tweet = re.sub(r'\\b\\w{1,2}\\b+', '', tweet) #get rid of all words <= 2 characters\n",
    "    #Tolkenize the Text\n",
    "    word_tokens = word_tokenize(tweet)\n",
    "    word_tokens = [w for w in word_tokens if not w in stop_words]\n",
    "    word_tokens = [w for w in word_tokens if not w in contractions]\n",
    "    \n",
    "    #tknzr = nltk.tokenize.casual.TweetTokenizer(preserve_case = False, strip_handles = True, reduce_len = True)\n",
    "    #tweet = tknzr.tokenize(tweet)\n",
    "    return word_tokens \n",
    "\n",
    "train['cleaned_text'] = train.apply(cleaning, axis = 1)\n",
    "test['cleaned_text'] = test.apply(cleaning, axis = 1)\n",
    "\n",
    "\n",
    "# Return text to string for processing later\n",
    "train['cleaned_text'] = train['cleaned_text'].apply(\" \".join)\n",
    "test['cleaned_text'] = test['cleaned_text'].apply(\" \".join)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c340bd27-c6f5-4176-b60f-ed38427122bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 text  target  \\\n",
      "0   Our Deeds are the Reason of this #earthquake M...       1   \n",
      "1              Forest fire near La Ronge Sask. Canada       1   \n",
      "2   All residents asked to 'shelter in place' are ...       1   \n",
      "3   13,000 people receive #wildfires evacuation or...       1   \n",
      "4   Just got sent this photo from Ruby #Alaska as ...       1   \n",
      "5   #RockyFire Update => California Hwy. 20 closed...       1   \n",
      "6   #flood #disaster Heavy rain causes flash flood...       1   \n",
      "7   I'm on top of the hill and I can see a fire in...       1   \n",
      "8   There's an emergency evacuation happening now ...       1   \n",
      "9   I'm afraid that the tornado is coming to our a...       1   \n",
      "10        Three people died from the heat wave so far       1   \n",
      "11  Haha South Tampa is getting flooded hah- WAIT ...       1   \n",
      "12  #raining #flooding #Florida #TampaBay #Tampa 1...       1   \n",
      "13            #Flood in Bago Myanmar #We arrived Bago       1   \n",
      "14  Damage to school bus on 80 in multi car crash ...       1   \n",
      "15                                     What's up man?       0   \n",
      "16                                      I love fruits       0   \n",
      "17                                   Summer is lovely       0   \n",
      "18                                  My car is so fast       0   \n",
      "19                       What a goooooooaaaaaal!!!!!!       0   \n",
      "\n",
      "                                         cleaned_text  \n",
      "0               deeds reason earthquake allah forgive  \n",
      "1                       forest fire ronge sask canada  \n",
      "2   residents asked shelter place notified officer...  \n",
      "3   people receive wildfires evacuation orders cal...  \n",
      "4      photo ruby alaska smoke wildfires pours school  \n",
      "5   rockyfire update california hwy closed directi...  \n",
      "6   flood disaster heavy rain flash flooding stree...  \n",
      "7                                 top hill fire woods  \n",
      "8      emergency evacuation happening building street  \n",
      "9                          afraid tornado coming area  \n",
      "10                              people died heat wave  \n",
      "11  haha south tampa flooded hah wait live south t...  \n",
      "12  raining flooding florida tampabay tampa days l...  \n",
      "13                    flood bago myanmar arrived bago  \n",
      "14         damage school bus multi car crash breaking  \n",
      "15                                                man  \n",
      "16                                        love fruits  \n",
      "17                                      summer lovely  \n",
      "18                                           car fast  \n",
      "19                                    goooooooaaaaaal  \n",
      "Rows processed with 0 remaining words\n",
      "                                                   text  target cleaned_text\n",
      "30                                             The end!       0             \n",
      "428             who makes these? http://t.co/28t3NWHdKy       0             \n",
      "1664                                 @Collapsed thank u       0             \n",
      "3584   @_AsianShawtyy ?????????? I'm sorry. But I'm out       0             \n",
      "3680                         That usually NEVER happens       0             \n",
      "4297  @HellFire_eV @JackPERU1 then I do this to one ...       0             \n",
      "4891                        @eileenmfl are you serious?       0             \n",
      "5564                          @NathanFillion Hardly! ??       0             \n"
     ]
    }
   ],
   "source": [
    "print(train.head(20))\n",
    "zero_length_lists = train[train['cleaned_text'].apply(lambda x: len(x) == 0)]\n",
    "print(\"Rows processed with 0 remaining words\")\n",
    "print(zero_length_lists)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f39157-7b29-4dd7-8141-e0c47d25ca2b",
   "metadata": {},
   "source": [
    "# Additional EDA.  \n",
    "Thats intresting.  All the items that are all stopwords are going to be no an emergency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "09b96d91-7fc7-4b72-8a95-39c8ffe6dd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets\n",
    "train_dat = train[['cleaned_text','target']].sample(frac = .9)\n",
    "test_dat = train[['cleaned_text','target']].drop(train_dat.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8d2e21-4a02-4eac-8734-655aa3fe1cba",
   "metadata": {},
   "source": [
    "Vectorizing\n",
    "\n",
    "I used Keras internal vectorizer to vectorize the text. I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7afd6bf9-dbf2-4f61-9b46-362fe1409a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tf_data = tf.data.Dataset.from_tensor_slices(\n",
    "    (train_dat['cleaned_text'].values, \n",
    "     train_dat['target'].values))\n",
    "test_tf_data = tf.data.Dataset.from_tensor_slices(\n",
    "    (test_dat['cleaned_text'].values, \n",
    "     test_dat['target'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bcaa7127-390c-4d7c-b398-3ffca8d34850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['' '[UNK]' 'amp' 'fire' 'news' 'people' 'don' 'video' 'disaster'\n",
      " 'emergency' 'police' 'body' 'california' 'time' 'storm' 'day' 'crash'\n",
      " 'burning' 'man' 'suicide']\n"
     ]
    }
   ],
   "source": [
    "# https://www.tensorflow.org/text/tutorials/text_classification_rnn\n",
    "\n",
    "VOCAB_SIZE = 1000\n",
    "encoder = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE)\n",
    "encoder.adapt(train_tf_data.map(lambda text, label: text))\n",
    "vocab = np.array(encoder.get_vocabulary())\n",
    "print(vocab[:20])\n",
    "#encoded_example = encoder(train_tf_data)[:3].numpy()\n",
    "#print(encoded_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5dfb92e5-e1e2-465a-bc51-d2ab00db8c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/text/tutorials/text_classification_rnn\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    encoder,\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=len(encoder.get_vocabulary()),\n",
    "        output_dim=64,\n",
    "        # Use masking to handle the variable sequence lengths\n",
    "        mask_zero=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5a50b767-275a-49cf-9da2-8bc1d7c42e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d722c50e-f405-40a6-a9ef-cd960ec0a535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[64], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_tf_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_tf_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest Loss:\u001b[39m\u001b[38;5;124m'\u001b[39m, test_loss)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest Accuracy:\u001b[39m\u001b[38;5;124m'\u001b[39m, test_acc)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    854\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    855\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    856\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 857\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    859\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    860\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_tf_data, epochs=10,\n",
    "                    validation_data=test_tf_data,\n",
    "                    validation_steps=30)\n",
    "print('Test Loss:', test_loss)\n",
    "print('Test Accuracy:', test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a62926a-c431-4701-bd68-030031eb1562",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
