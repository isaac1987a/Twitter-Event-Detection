{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1fb9b2ba-cc97-41cb-8336-809a53c680ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f13d76ab-a8f6-4c3e-b473-6a85fd09b0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch loads\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data_utils\n",
    "\n",
    "# Non-Torch Loads\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Cleaning Loads\n",
    "import regex as re\n",
    "import emoji\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "04af0667-ee53-4803-af07-ede8f010b263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  target\n",
      "0  Our Deeds are the Reason of this #earthquake M...       1\n",
      "1             Forest fire near La Ronge Sask. Canada       1\n",
      "2  All residents asked to 'shelter in place' are ...       1\n",
      "3  13,000 people receive #wildfires evacuation or...       1\n",
      "4  Just got sent this photo from Ruby #Alaska as ...       1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7613, 2)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"nlp-getting-started/train.csv\")\n",
    "train = train.drop(labels = [\"keyword\", \"location\", \"id\"], axis = 1)\n",
    "print(train.head())\n",
    "(train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81fc971-3d4e-4ede-9035-b4a4375b7b0a",
   "metadata": {},
   "source": [
    "# EDA\n",
    "The dataset has 57% non-disaster tweets, and 43% Disaster tweets.  There are 31924 unique words.  This will drive my tuning the vectorization of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4b938ef7-8812-4bd8-b1b9-79b0560ff05a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    7613.00000\n",
       "mean        0.42966\n",
       "std         0.49506\n",
       "min         0.00000\n",
       "25%         0.00000\n",
       "50%         0.00000\n",
       "75%         1.00000\n",
       "max         1.00000\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['target'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5250c13b-716d-4f91-86a8-221905096093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31924\n"
     ]
    }
   ],
   "source": [
    "unique_words = set()\n",
    "for text in train[\"text\"]:\n",
    "    unique_words.update(text.split())\n",
    "print(len(unique_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5248ba15-caf2-4a97-af6e-2d0dfabe2850",
   "metadata": {},
   "source": [
    "# Cleaning\n",
    "Standard tweet cleaning.  Cleaning found at:\n",
    "https://stackoverflow.com/questions/64719706/cleaning-twitter-data-pandas-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d1b87ba1-c57f-4159-8c9f-e60f9c8f5365",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tweet in train['text']:\n",
    "    tweet = re.sub(\"@[A-Za-z0-9]+\",\"\",tweet) #Remove @ sign\n",
    "    tweet = re.sub(r\"(?:\\@|http?\\://|https?\\://|www)\\S+\", \"\", tweet) #Remove http links\n",
    "    tweet = \" \".join(tweet.split())\n",
    "    tweet = emoji.replace_emoji(tweet, '') #Remove Emojis\n",
    "    tweet = tweet.replace(\"#\", \"\").replace(\"_\", \" \") #Remove hashtag sign but keep the text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17c0649-e1af-40ff-8d42-3ab6b7b138af",
   "metadata": {},
   "source": [
    "# Vectorizing\n",
    "I used pytorchs internal vectorizer to vectorize the text.  I\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9a443764-d66c-45fa-8d15-d3cb05df622c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\isaac\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\isaac\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\utils\\_runtime.py:184: UserWarning: Pydantic is installed but cannot be imported. Please check your installation. `huggingface_hub` will default to not using Pydantic. Error message: '{e}'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Tweets(\n",
       "  (embedding): Embedding(20000, 300)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initial Setup\n",
    "\n",
    "class Tweets(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(Tweets, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # Add other layers as needed\n",
    "\n",
    "    def forward(self, input_text):\n",
    "        embedded = self.embedding(input_text)\n",
    "        # Additional model operations\n",
    "        return embedded\n",
    "\n",
    "\n",
    "\n",
    "model = Tweets(vocab_size=20000, embedding_dim=300)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "model.eval()\n",
    "\n",
    "# import data to Torch\n",
    "train = data_utils.TensorDataset("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca084162-e5c6-4c64-aaf9-ffafc0ccc723",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
