{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1fb9b2ba-cc97-41cb-8336-809a53c680ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f13d76ab-a8f6-4c3e-b473-6a85fd09b0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch loads\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data_utils\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Non-Torch Loads\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "#Cleaning Loads\n",
    "import regex as re\n",
    "import emoji\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "04af0667-ee53-4803-af07-ede8f010b263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  target\n",
      "0  Our Deeds are the Reason of this #earthquake M...       1\n",
      "1             Forest fire near La Ronge Sask. Canada       1\n",
      "2  All residents asked to 'shelter in place' are ...       1\n",
      "3  13,000 people receive #wildfires evacuation or...       1\n",
      "4  Just got sent this photo from Ruby #Alaska as ...       1\n",
      "(7613, 2)\n",
      "Index(['text', 'target'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"nlp-getting-started/train.csv\", header = 0)\n",
    "train = train.drop(labels = [\"keyword\", \"location\", \"id\"], axis = 1)\n",
    "print(train.head())\n",
    "print(train.shape)\n",
    "print(train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81fc971-3d4e-4ede-9035-b4a4375b7b0a",
   "metadata": {},
   "source": [
    "# EDA\n",
    "The dataset has 57% non-disaster tweets, and 43% Disaster tweets.  There are 31924 unique words.  This will drive my tuning the vectorization of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4b938ef7-8812-4bd8-b1b9-79b0560ff05a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    7613.00000\n",
       "mean        0.42966\n",
       "std         0.49506\n",
       "min         0.00000\n",
       "25%         0.00000\n",
       "50%         0.00000\n",
       "75%         1.00000\n",
       "max         1.00000\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['target'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5250c13b-716d-4f91-86a8-221905096093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31924\n"
     ]
    }
   ],
   "source": [
    "unique_words = set()\n",
    "for text in train[\"text\"]:\n",
    "    unique_words.update(text.split())\n",
    "print(len(unique_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5248ba15-caf2-4a97-af6e-2d0dfabe2850",
   "metadata": {},
   "source": [
    "# Cleaning\n",
    "Standard tweet cleaning.  Cleaning found at:\n",
    "https://stackoverflow.com/questions/64719706/cleaning-twitter-data-pandas-python\n",
    "\n",
    "# Tokenizatin\n",
    "I tolkenized the tweets in preparation to convert to tensors for embedding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d1b87ba1-c57f-4159-8c9f-e60f9c8f5365",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vocab = nltk.lm.Vocabulary()\n",
    "output = pd.DataFrame()\n",
    "def cleaning(line):\n",
    "    tweet = line['text']\n",
    "    tweet = re.sub(\"@[A-Za-z0-9]+\",\"\",tweet) #Remove @ sign\n",
    "    tweet = re.sub(r\"(?:\\@|http?\\://|https?\\://|www)\\S+\", \"\", tweet) #Remove http links\n",
    "    tweet = \" \".join(tweet.split())\n",
    "    tweet = emoji.replace_emoji(tweet, '') #Remove Emojis\n",
    "    tweet = tweet.replace(\"#\", \"\").replace(\"_\", \" \") #Remove hashtag sign but keep the text\n",
    "    tweet = re.sub(r'[^a-z]', ' ', tweet) # Strip all symbols and replace with +\n",
    "    #tweet = re.sub(r'[\\w{3,}]+', '', tweet) #get rid of all words <= 2 characters\n",
    "    #Tolkenize the Text\n",
    "    tweet = tweet.lower()\n",
    "    vocab.update(tweet.split(\" \"))\n",
    "    #tknzr = nltk.tokenize.casual.TweetTokenizer(preserve_case = False, strip_handles = True, reduce_len = True)\n",
    "    #tweet = tknzr.tokenize(tweet)\n",
    "    return tweet \n",
    "train['cleaned_text'] = train.apply(cleaning, axis = 1)\n",
    "train['text'] = train['cleaned_text']\n",
    "train = train.drop(labels = ['cleaned_text'], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "545afe54-0a63-4077-b8d1-378bccc2abee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(list(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "38e526e0-5b4a-44ab-93bd-d350e646191b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets\n",
    "train_dat = train.sample(frac = .9)\n",
    "test_dat = train.drop(train_dat.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "45c83554-d800-4f68-9d79-30a1c5cd9658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14665\n"
     ]
    }
   ],
   "source": [
    "vocab_dict = {}\n",
    "for item in list(vocab):\n",
    "    vocab_dict[item] = vocab[item]\n",
    "print(len(vocab_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17c0649-e1af-40ff-8d42-3ab6b7b138af",
   "metadata": {},
   "source": [
    "# Vectorizing\n",
    "I used pytorchs internal vectorizer to vectorize the text.  I\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "227296e5-b28c-4a61-81c5-d47a90151761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Importing from https://pytorch.org/tutorials/beginner/basics/data_tutorial.html?highlight=dataloader\n",
    "\n",
    "#tokenize the text for tensor load\n",
    "def tokenize(text):\n",
    "    # Your tokenizer logic here\n",
    "    return [vocab[token] for token in text.split()]\n",
    "\n",
    "class CustomTextDataset(Dataset):\n",
    "    def __init__(self, data,vocab, transform=None, target_transform=None):\n",
    "        self.data = data\n",
    "        self.vocab = vocab\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tweet = self.data.iloc[idx, 0]\n",
    "        label = self.data.iloc[idx, 1]\n",
    "        tokenized_tweet = [self.vocab.get(token, 0) for token in tweet.split()]  # Tokenize and handle unknown tokens\n",
    "        tweet_tensor  = torch.tensor(tokenized_tweet, dtype=torch.long)\n",
    "        label_tensor = torch.tensor(label, dtype=torch.float)\n",
    "        \n",
    "        if self.transform:\n",
    "            tweet_tensor = self.transform(tweet_tensor)\n",
    "        if self.target_transform:\n",
    "            label_tensor = self.target_transform(label_tensor)\n",
    "\n",
    "        return tweet_tensor, label_tensor\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list = [], []\n",
    "    for (_text, _label) in batch:\n",
    "        label_list.append(_label)\n",
    "        text_list.append(_text.clone().detach())\n",
    "    return pad_sequence(text_list, batch_first=True), torch.tensor(label_list, dtype=torch.float)\n",
    "\n",
    "# Creating training and validation datasets\n",
    "training_set = CustomTextDataset(train_dat, vocab_dict)\n",
    "validation_set = CustomTextDataset(test_dat, vocab_dict)\n",
    "#test set = CustomTextDataset(val_dat, vocab)\n",
    "\n",
    "# Creating data loaders\n",
    "training_loader = torch.utils.data.DataLoader(training_set, batch_size=32, shuffle=True, collate_fn=collate_batch)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=32, shuffle=False, collate_fn=collate_batch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2b0482-3bc0-4937-bc04-21171bef4d13",
   "metadata": {},
   "source": [
    "# Model approach\n",
    "I'm choosing to use a RNN were I only select the last value as the output, so many in and 1 out.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9a443764-d66c-45fa-8d15-d3cb05df622c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial Setup\n",
    "\n",
    "class Tweets(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers, nonlinearity):\n",
    "        super(Tweets, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn_layer = nn.RNN(input_size=embedding_dim, hidden_size=hidden_size, num_layers=num_layers,\n",
    "                                nonlinearity=nonlinearity, bias=True, bidirectional=False)\n",
    "        self.dropout = nn.Dropout(p=.75)\n",
    "        #self.relu = nn.ReLU()\n",
    "        self.linear = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "        #self.output_layer = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_text):\n",
    "        # Assuming input_text is already a LongTensor that has been prepared outside the model\n",
    "        embedded = self.embedding(input_text)\n",
    "        rnn_output, _ = self.rnn_layer(embedded)\n",
    "        dropout = self.dropout(rnn_output)\n",
    "        output = dropout[:, :, :]  # Extract the last timestep output\n",
    "        linear = self.linear(output)\n",
    "        final_output = linear.mean(dim=1)\n",
    "        #print(final_output.shape)\n",
    "        return final_output.squeeze()  # Squeeze to remove the extra dimension\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5090bbb-2db1-4606-bec5-dc878ab4a289",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ca084162-e5c6-4c64-aaf9-ffafc0ccc723",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_one_epoch(epoch_index, model,training_loader, loss_fn, optimizer, tb_writer):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, data in enumerate(training_loader):\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, labels = data\n",
    "        inputs_cuda = inputs.to('cuda')\n",
    "        labels_cuda = labels.to('cuda')\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs = model(inputs_cuda)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_fn(outputs, labels_cuda)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 9:\n",
    "            last_loss = running_loss / 1000 # loss per batch\n",
    "            #print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            tb_x = epoch_index * len(training_loader) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "\n",
    "    return last_loss\n",
    "\n",
    "# test execution\n",
    "#model = Tweets(vocab_size=30000, embedding_dim =1024).to('cuda')\n",
    "#loss_fn = nn.BCEWithLogitsLoss()\n",
    "#optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "#tb_writer = SummaryWriter('logs')\n",
    "#train_one_epoch(1,model, training_loader, loss_fn, optimizer, tb_writer)\n",
    "#tb_writer.close()\n",
    "\n",
    "output_data = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e25815-0179-4867-bf2d-87317bd2bbce",
   "metadata": {},
   "source": [
    "Ok, .68 validation loss.  Thats not good.  I'm going to run a optomizer to try and find the optimal variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a65f7ffd-cba2-453d-a940-670aac21f118",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "def run_model(vocab_size, embedding_dim,\n",
    "              hidden_size, num_layers, nonlinearity, output_data, epochs = 10):\n",
    "\n",
    "    \n",
    "    model = Tweets(vocab_size=vocab_size, embedding_dim = embedding_dim,\n",
    "                   hidden_size = hidden_size, \n",
    "                   num_layers= num_layers, nonlinearity = nonlinearity).to('cuda')\n",
    "    \n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "    \n",
    "    timestamp = datetime.now(timezone.utc).strftime('%Y%m%d_%H%M%S')\n",
    "    writer = SummaryWriter('runs/fashion_trainer_{}'.format(timestamp))\n",
    "    \n",
    "    epoch_number = 0\n",
    "    \n",
    "    best_vloss = 1_000_000.\n",
    "    \n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print('EPOCH {}:'.format(epoch_number + 1))\n",
    "    \n",
    "        # Make sure gradient tracking is on, and do a pass over the data\n",
    "        model.train(True)\n",
    "        avg_loss = train_one_epoch(epoch_number,model, training_loader, loss_fn, optimizer, writer)\n",
    "    \n",
    "    \n",
    "        running_vloss = 0.0\n",
    "        # Set the model to evaluation mode, disabling dropout and using population\n",
    "        # statistics for batch normalization.\n",
    "        model.eval()\n",
    "    \n",
    "        # Disable gradient computation and reduce memory consumption.\n",
    "        with torch.no_grad():\n",
    "            for i, vdata in enumerate(validation_loader):\n",
    "                vinputs, vlabels = vdata\n",
    "                vinputs_cuda = vinputs.to('cuda')\n",
    "                vlabels_cuda = vlabels.to('cuda')\n",
    "                voutputs = model(vinputs_cuda)\n",
    "                \n",
    "                voutputs = model(vinputs_cuda)\n",
    "                vloss = loss_fn(voutputs, vlabels_cuda)\n",
    "                running_vloss += vloss\n",
    "    \n",
    "        avg_vloss = running_vloss / (i + 1)\n",
    "        print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "        \n",
    "        # Log the running loss averaged per batch\n",
    "        # for both training and validation\n",
    "        writer.add_scalars('Training vs. Validation Loss',\n",
    "                        { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                        epoch_number + 1)\n",
    "        writer.flush()\n",
    "        \n",
    "        # Output results for charting\n",
    "        new_row = {\n",
    "            \"vocab_size\": vocab_size,\n",
    "            \"embedding_dim\": embedding_dim,\n",
    "            \"hidden_size\": hidden_size,\n",
    "            \"num_layers\": num_layers,\n",
    "            \"loss_fn\": str(loss_fn),\n",
    "            \"optimizer\": str(optimizer),\n",
    "            \"run\": run,\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": str(avg_loss),\n",
    "            \"val_loss\": str(avg_vloss),\n",
    "            \"nonlinearity\": nonlinearity\n",
    "        }\n",
    "        output_data = pd.concat([output_data, pd.Series(new_row)], ignore_index=True)\n",
    "        # Track best performance, and save the model's state\n",
    "        if avg_vloss < best_vloss:\n",
    "            best_vloss = avg_vloss\n",
    "            model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
    "            #torch.save(model.state_dict(), model_path)\n",
    "    \n",
    "        epoch_number += 1\n",
    "    print(f'Best Validation Loss {best_vloss}')\n",
    "    torch.cuda.empty_cache()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d8aab295-5a3b-41e9-bf5f-ff9ad7ce896d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "LOSS train 0.006737415671348571 valid 0.6773178577423096\n",
      "EPOCH 2:\n",
      "LOSS train 0.0066872814297676084 valid 0.6698739528656006\n",
      "EPOCH 3:\n",
      "LOSS train 0.006597165942192077 valid 0.6575964689254761\n",
      "EPOCH 4:\n",
      "LOSS train 0.006508150577545166 valid 0.6427116990089417\n",
      "EPOCH 5:\n",
      "LOSS train 0.006447979867458343 valid 0.6270498037338257\n",
      "EPOCH 6:\n",
      "LOSS train 0.006156076610088349 valid 0.6130244731903076\n",
      "EPOCH 7:\n",
      "LOSS train 0.005974364697933197 valid 0.6045231819152832\n",
      "EPOCH 8:\n",
      "LOSS train 0.00631960329413414 valid 0.5952804088592529\n",
      "EPOCH 9:\n",
      "LOSS train 0.006017493426799774 valid 0.592774510383606\n",
      "EPOCH 10:\n",
      "LOSS train 0.005771614491939545 valid 0.588695764541626\n",
      "EPOCH 11:\n",
      "LOSS train 0.005776144176721573 valid 0.5893983840942383\n",
      "EPOCH 12:\n",
      "LOSS train 0.00584258359670639 valid 0.595373809337616\n",
      "EPOCH 13:\n",
      "LOSS train 0.005733406633138657 valid 0.5871719717979431\n",
      "EPOCH 14:\n",
      "LOSS train 0.005535050421953201 valid 0.604561448097229\n",
      "EPOCH 15:\n",
      "LOSS train 0.00584706711769104 valid 0.5810458660125732\n",
      "EPOCH 16:\n",
      "LOSS train 0.005790261030197144 valid 0.5846649408340454\n",
      "EPOCH 17:\n",
      "LOSS train 0.00587938243150711 valid 0.5839149951934814\n",
      "EPOCH 18:\n",
      "LOSS train 0.005852383702993393 valid 0.5898904800415039\n",
      "EPOCH 19:\n",
      "LOSS train 0.005559462249279022 valid 0.5803908109664917\n",
      "EPOCH 20:\n",
      "LOSS train 0.0054482978582382205 valid 0.5811976194381714\n",
      "Best Validation Loss 0.5803908109664917\n"
     ]
    }
   ],
   "source": [
    "run = 1\n",
    "embedding_dim = 64\n",
    "hidden_size = 64\n",
    "num_layers = 1\n",
    "vocab_size = 64\n",
    "nonlinearity = 'tanh'\n",
    "model = run_model(vocab_size = len(vocab_dict), \n",
    "                  embedding_dim = embedding_dim,\n",
    "                  hidden_size = hidden_size, \n",
    "                  num_layers= num_layers, \n",
    "                  nonlinearity = nonlinearity,\n",
    "                  output_data = output_data,\n",
    "                  epochs = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9198e05-81c1-4408-85a2-754b4916ea12",
   "metadata": {},
   "source": [
    "This is some very aggressive overfitting.  After research, the embedding DIM is usually between 1 and 300.  I'll try shrinking that.  I'll also shrink the hidden size to 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "698a4c68-d6db-441d-afd7-270c3dfb9b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "LOSS train 0.0066006632447242735 valid 0.6659048199653625\n",
      "EPOCH 2:\n",
      "LOSS train 0.00641293329000473 valid 0.651095986366272\n",
      "EPOCH 3:\n",
      "LOSS train 0.006378297686576843 valid 0.6264338493347168\n",
      "EPOCH 4:\n",
      "LOSS train 0.006140410900115967 valid 0.6097590327262878\n",
      "EPOCH 5:\n",
      "LOSS train 0.006155564665794372 valid 0.601556658744812\n",
      "EPOCH 6:\n",
      "LOSS train 0.006025470733642578 valid 0.5853719711303711\n",
      "EPOCH 7:\n",
      "LOSS train 0.005687238365411759 valid 0.5822876691818237\n",
      "EPOCH 8:\n",
      "LOSS train 0.005852213084697723 valid 0.5750299692153931\n",
      "EPOCH 9:\n",
      "LOSS train 0.005674470394849777 valid 0.5734764933586121\n",
      "EPOCH 10:\n",
      "LOSS train 0.0059062220454216005 valid 0.5759503245353699\n",
      "EPOCH 11:\n",
      "LOSS train 0.005705447643995285 valid 0.5779459476470947\n",
      "EPOCH 12:\n",
      "LOSS train 0.005604698300361633 valid 0.5784727931022644\n",
      "EPOCH 13:\n",
      "LOSS train 0.006115310966968536 valid 0.5824260115623474\n",
      "EPOCH 14:\n",
      "LOSS train 0.005391271531581879 valid 0.5769169330596924\n",
      "EPOCH 15:\n",
      "LOSS train 0.005829308271408081 valid 0.5875175595283508\n",
      "EPOCH 16:\n",
      "LOSS train 0.005708035618066788 valid 0.5754500031471252\n",
      "EPOCH 17:\n",
      "LOSS train 0.005955436557531357 valid 0.5735292434692383\n",
      "EPOCH 18:\n",
      "LOSS train 0.00547588974237442 valid 0.579780101776123\n",
      "EPOCH 19:\n",
      "LOSS train 0.005879310548305512 valid 0.575402021408081\n",
      "EPOCH 20:\n",
      "LOSS train 0.005755105793476105 valid 0.5786569714546204\n",
      "Best Validation Loss 0.5734764933586121\n"
     ]
    }
   ],
   "source": [
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "\n",
    "run = run+1\n",
    "\n",
    "embedding_dim = 128\n",
    "hidden_size = 128\n",
    "num_layers = 1\n",
    "vocab_size = 30000\n",
    "nonlinearity = 'tanh'\n",
    "\n",
    "model = run_model(vocab_size = len(vocab_dict), \n",
    "                  embedding_dim = embedding_dim,\n",
    "                  hidden_size = hidden_size, \n",
    "                  num_layers= num_layers, \n",
    "                  nonlinearity = nonlinearity,\n",
    "                  output_data = output_data,\n",
    "                  epochs = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164bf680-7a41-426e-b73e-6dcdbad7e926",
   "metadata": {},
   "source": [
    "No Joy.  I'm going to switch from tanh to something else.  I'll try ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d587be09-2aef-4597-9fff-b70a3c158bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "LOSS train 0.006768856287002563 valid 0.6833497285842896\n",
      "EPOCH 2:\n",
      "LOSS train 0.006859190702438354 valid 0.680534839630127\n",
      "EPOCH 3:\n",
      "LOSS train 0.00674616813659668 valid 0.6781796216964722\n",
      "EPOCH 4:\n",
      "LOSS train 0.006859686195850373 valid 0.6732800006866455\n",
      "EPOCH 5:\n",
      "LOSS train 0.006767591953277588 valid 0.6681431531906128\n",
      "EPOCH 6:\n",
      "LOSS train 0.006643198132514953 valid 0.6611102819442749\n",
      "EPOCH 7:\n",
      "LOSS train 0.0063378349542617795 valid 0.65326327085495\n",
      "EPOCH 8:\n",
      "LOSS train 0.006465935587882996 valid 0.6417407989501953\n",
      "EPOCH 9:\n",
      "LOSS train 0.006419484615325928 valid 0.631295382976532\n",
      "EPOCH 10:\n",
      "LOSS train 0.006036305785179138 valid 0.6238367557525635\n",
      "EPOCH 11:\n",
      "LOSS train 0.005908311903476715 valid 0.6109274625778198\n",
      "EPOCH 12:\n",
      "LOSS train 0.006298208296298981 valid 0.6023268103599548\n",
      "EPOCH 13:\n",
      "LOSS train 0.0063184854388237 valid 0.5930426716804504\n",
      "EPOCH 14:\n",
      "LOSS train 0.005994602084159851 valid 0.5899097323417664\n",
      "EPOCH 15:\n",
      "LOSS train 0.0056389237940311435 valid 0.5910115242004395\n",
      "EPOCH 16:\n",
      "LOSS train 0.0056250101625919344 valid 0.5836448669433594\n",
      "EPOCH 17:\n",
      "LOSS train 0.0055672434568405154 valid 0.5802377462387085\n",
      "EPOCH 18:\n",
      "LOSS train 0.00632019978761673 valid 0.5772897005081177\n",
      "EPOCH 19:\n",
      "LOSS train 0.00586523848772049 valid 0.5762914419174194\n",
      "EPOCH 20:\n",
      "LOSS train 0.005954318523406983 valid 0.5753419399261475\n",
      "Best Validation Loss 0.5753419399261475\n"
     ]
    }
   ],
   "source": [
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "\n",
    "run = run+1\n",
    "\n",
    "embedding_dim = 32\n",
    "hidden_size = 32\n",
    "num_layers = 1\n",
    "nonlinearity = 'relu'\n",
    "\n",
    "model = run_model(vocab_size = len(vocab_dict), \n",
    "                  embedding_dim = embedding_dim,\n",
    "                  hidden_size = hidden_size, \n",
    "                  num_layers= num_layers, \n",
    "                  nonlinearity = nonlinearity,\n",
    "                  output_data = output_data,\n",
    "                  epochs = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62cdd68-25d4-4d27-a57e-57fc492b8c51",
   "metadata": {},
   "source": [
    "Not alot of learning going due to overfitting.  I was hoping that RelU would fix this.  I'm shrinking the vocab size and trying again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d4c0eba6-78b9-4015-883a-0caaf19b421c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "LOSS train 0.006884817123413086 valid 0.6820456385612488\n",
      "EPOCH 2:\n",
      "LOSS train 0.006775200486183166 valid 0.6784233450889587\n",
      "EPOCH 3:\n",
      "LOSS train 0.006733012616634369 valid 0.6708706617355347\n",
      "EPOCH 4:\n",
      "LOSS train 0.006508873879909515 valid 0.6584452390670776\n",
      "EPOCH 5:\n",
      "LOSS train 0.0064526451230049135 valid 0.6398960947990417\n",
      "EPOCH 6:\n",
      "LOSS train 0.0062542703151702884 valid 0.620219349861145\n",
      "EPOCH 7:\n",
      "LOSS train 0.0060516902208328244 valid 0.6071058511734009\n",
      "EPOCH 8:\n",
      "LOSS train 0.0063573973774909975 valid 0.6013250946998596\n",
      "EPOCH 9:\n",
      "LOSS train 0.005788935840129852 valid 0.5949277877807617\n",
      "EPOCH 10:\n",
      "LOSS train 0.005978150069713592 valid 0.5881658792495728\n",
      "EPOCH 11:\n",
      "LOSS train 0.005724472910165787 valid 0.5850135684013367\n",
      "EPOCH 12:\n",
      "LOSS train 0.005747364342212677 valid 0.5825825333595276\n",
      "EPOCH 13:\n",
      "LOSS train 0.005679448515176773 valid 0.5801957249641418\n",
      "EPOCH 14:\n",
      "LOSS train 0.006018897324800492 valid 0.5766094923019409\n",
      "EPOCH 15:\n",
      "LOSS train 0.005630888521671295 valid 0.5783380270004272\n",
      "EPOCH 16:\n",
      "LOSS train 0.0054213511943817135 valid 0.576648473739624\n",
      "EPOCH 17:\n",
      "LOSS train 0.006229798197746277 valid 0.5795136094093323\n",
      "EPOCH 18:\n",
      "LOSS train 0.005590781629085541 valid 0.5735819935798645\n",
      "EPOCH 19:\n",
      "LOSS train 0.005457081347703934 valid 0.5714761018753052\n",
      "EPOCH 20:\n",
      "LOSS train 0.00549090039730072 valid 0.571667492389679\n",
      "Best Validation Loss 0.5714761018753052\n"
     ]
    }
   ],
   "source": [
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "\n",
    "run = run+1\n",
    "\n",
    "embedding_dim = 32\n",
    "hidden_size = 32\n",
    "num_layers = 2\n",
    "nonlinearity = 'relu'\n",
    "\n",
    "model = run_model(vocab_size = len(vocab_dict), \n",
    "                  embedding_dim = embedding_dim,\n",
    "                  hidden_size = hidden_size, \n",
    "                  num_layers= num_layers, \n",
    "                  nonlinearity = nonlinearity,\n",
    "                  output_data = output_data,\n",
    "                  epochs = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbe3c38-fcbc-487e-9414-690adba569de",
   "metadata": {},
   "source": [
    "A bit counter intuitive, but I'll try to increase the number of hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c2c971f2-9997-4875-8473-39ec24d057bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "LOSS train 0.006865632236003876 valid 0.6830936670303345\n",
      "EPOCH 2:\n",
      "LOSS train 0.006851979553699494 valid 0.6799517869949341\n",
      "EPOCH 3:\n",
      "LOSS train 0.006662387192249298 valid 0.6714770793914795\n",
      "EPOCH 4:\n",
      "LOSS train 0.006688854575157165 valid 0.6555174589157104\n",
      "EPOCH 5:\n",
      "LOSS train 0.006489135205745697 valid 0.6390665769577026\n",
      "EPOCH 6:\n",
      "LOSS train 0.006297772765159607 valid 0.6252726912498474\n",
      "EPOCH 7:\n",
      "LOSS train 0.00639112389087677 valid 0.6120244860649109\n",
      "EPOCH 8:\n",
      "LOSS train 0.005783014297485352 valid 0.6005529165267944\n",
      "EPOCH 9:\n",
      "LOSS train 0.005871547162532806 valid 0.5932407379150391\n",
      "EPOCH 10:\n",
      "LOSS train 0.005842251688241959 valid 0.5905552506446838\n",
      "EPOCH 11:\n",
      "LOSS train 0.005498228996992111 valid 0.5869067907333374\n",
      "EPOCH 12:\n",
      "LOSS train 0.005487593919038773 valid 0.582530677318573\n",
      "EPOCH 13:\n",
      "LOSS train 0.005773815870285034 valid 0.5813212990760803\n",
      "EPOCH 14:\n",
      "LOSS train 0.005673999160528183 valid 0.579251766204834\n",
      "EPOCH 15:\n",
      "LOSS train 0.005907477021217346 valid 0.5778605937957764\n",
      "EPOCH 16:\n",
      "LOSS train 0.005491407513618469 valid 0.5775688290596008\n",
      "EPOCH 17:\n",
      "LOSS train 0.005506603121757507 valid 0.5763400793075562\n",
      "EPOCH 18:\n",
      "LOSS train 0.005291744917631149 valid 0.5741090774536133\n",
      "EPOCH 19:\n",
      "LOSS train 0.005737802565097809 valid 0.5712940692901611\n",
      "EPOCH 20:\n",
      "LOSS train 0.005163311362266541 valid 0.5733311176300049\n",
      "Best Validation Loss 0.5712940692901611\n"
     ]
    }
   ],
   "source": [
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "\n",
    "run = run+1\n",
    "\n",
    "embedding_dim = 32\n",
    "hidden_size = 32\n",
    "num_layers = 2\n",
    "vocab_size = 10000\n",
    "nonlinearity = 'relu'\n",
    "\n",
    "model = run_model(vocab_size = len(vocab_dict), \n",
    "                  embedding_dim = embedding_dim,\n",
    "                  hidden_size = hidden_size, \n",
    "                  num_layers= num_layers, \n",
    "                  nonlinearity = nonlinearity,\n",
    "                  output_data = output_data,\n",
    "                  epochs = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee99db9-5d6f-4f65-8c15-8bc6a5e5ab98",
   "metadata": {},
   "source": [
    "Going back, one of the advantages of the RNN is that is recursive.  I'll change that setting in the model setup and try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "937c1291-5073-4879-8633-52ccaeb98bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tweets(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers, nonlinearity):\n",
    "        super(Tweets, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn_layer = nn.RNN(input_size=embedding_dim, hidden_size=hidden_size, num_layers=num_layers,\n",
    "                                nonlinearity=nonlinearity, bias=True, bidirectional=True)\n",
    "        self.output_layer = nn.Linear(embedding_dim, 1)\n",
    "\n",
    "    def forward(self, input_text):\n",
    "        # Assuming input_text is already a LongTensor that has been prepared outside the model\n",
    "        embedded = self.embedding(input_text)\n",
    "        rnn_output, _ = self.rnn_layer(embedded)\n",
    "        output = rnn_output[:, -1, :]  # Extract the last timestep output\n",
    "        final_output = self.output_layer(output)\n",
    "        return final_output.squeeze()  # Squeeze to remove the extra dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9acc7dec-166b-4e58-b538-b025a0e02d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (32x64 and 32x1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[86], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m vocab_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10000\u001b[39m\n\u001b[0;32m      5\u001b[0m nonlinearity \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 7\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mrun_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvocab_dict\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m                  \u001b[49m\u001b[43membedding_dim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43membedding_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mnonlinearity\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnonlinearity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m                  \u001b[49m\u001b[43moutput_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moutput_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[75], line 27\u001b[0m, in \u001b[0;36mrun_model\u001b[1;34m(vocab_size, embedding_dim, hidden_size, num_layers, nonlinearity, output_data, epochs)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Make sure gradient tracking is on, and do a pass over the data\u001b[39;00m\n\u001b[0;32m     26\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 27\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_number\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m running_vloss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Set the model to evaluation mode, disabling dropout and using population\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# statistics for batch normalization.\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[74], line 17\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(epoch_index, model, training_loader, loss_fn, optimizer, tb_writer)\u001b[0m\n\u001b[0;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Make predictions for this batch\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs_cuda\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Compute the loss and its gradients\u001b[39;00m\n\u001b[0;32m     20\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(outputs, labels_cuda)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[85], line 14\u001b[0m, in \u001b[0;36mTweets.forward\u001b[1;34m(self, input_text)\u001b[0m\n\u001b[0;32m     12\u001b[0m rnn_output, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrnn_layer(embedded)\n\u001b[0;32m     13\u001b[0m output \u001b[38;5;241m=\u001b[39m rnn_output[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]  \u001b[38;5;66;03m# Extract the last timestep output\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m final_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m final_output\u001b[38;5;241m.\u001b[39msqueeze()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (32x64 and 32x1)"
     ]
    }
   ],
   "source": [
    "embedding_dim = 32\n",
    "hidden_size = 32\n",
    "num_layers = 1\n",
    "vocab_size = 10000\n",
    "nonlinearity = 'relu'\n",
    "\n",
    "model = run_model(vocab_size = len(vocab_dict), \n",
    "                  embedding_dim = embedding_dim,\n",
    "                  hidden_size = hidden_size, \n",
    "                  num_layers= num_layers, \n",
    "                  nonlinearity = nonlinearity,\n",
    "                  output_data = output_data,\n",
    "                  epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdc04e6-f105-40ae-b45e-0427741369c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
