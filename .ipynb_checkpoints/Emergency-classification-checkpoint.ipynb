{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fb9b2ba-cc97-41cb-8336-809a53c680ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f13d76ab-a8f6-4c3e-b473-6a85fd09b0c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\isaac\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "# Torch loads\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data_utils\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from torcheval.metrics.functional import binary_accuracy\n",
    "\n",
    "# Non-Torch Loads\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "#Cleaning Loads\n",
    "import regex as re\n",
    "import emoji\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import requests\n",
    "\n",
    "#Visualization\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04af0667-ee53-4803-af07-ede8f010b263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "                                                text  target\n",
      "0  Our Deeds are the Reason of this #earthquake M...       1\n",
      "1             Forest fire near La Ronge Sask. Canada       1\n",
      "2  All residents asked to 'shelter in place' are ...       1\n",
      "3  13,000 people receive #wildfires evacuation or...       1\n",
      "4  Just got sent this photo from Ruby #Alaska as ...       1\n",
      "test\n",
      "   id                                               text\n",
      "0   0                 Just happened a terrible car crash\n",
      "1   2  Heard about #earthquake is different cities, s...\n",
      "2   3  there is a forest fire at spot pond, geese are...\n",
      "3   9           Apocalypse lighting. #Spokane #wildfires\n",
      "4  11      Typhoon Soudelor kills 28 in China and Taiwan\n"
     ]
    }
   ],
   "source": [
    "train_path = \"nlp-getting-started/train.csv\"\n",
    "test_path = \"nlp-getting-started/test.csv\"\n",
    "train = pd.read_csv(train_path, header = 0)\n",
    "train = train.drop(labels = [\"keyword\", \"location\", \"id\"], axis = 1)\n",
    "test = pd.read_csv(test_path, header = 0)\n",
    "test = test.drop(labels = [\"keyword\", \"location\"], axis = 1)\n",
    "print(\"train\")\n",
    "print(train.head())\n",
    "print(\"test\")\n",
    "print(test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81fc971-3d4e-4ede-9035-b4a4375b7b0a",
   "metadata": {},
   "source": [
    "# EDA\n",
    "The dataset has 57% non-disaster tweets, and 43% Disaster tweets.  There are 31924 unique words.  This will drive my tuning the vectorization of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b938ef7-8812-4bd8-b1b9-79b0560ff05a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    7613.00000\n",
       "mean        0.42966\n",
       "std         0.49506\n",
       "min         0.00000\n",
       "25%         0.00000\n",
       "50%         0.00000\n",
       "75%         1.00000\n",
       "max         1.00000\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['target'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5250c13b-716d-4f91-86a8-221905096093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31924\n"
     ]
    }
   ],
   "source": [
    "unique_words = set()\n",
    "for text in train[\"text\"]:\n",
    "    unique_words.update(text.split())\n",
    "print(len(unique_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5248ba15-caf2-4a97-af6e-2d0dfabe2850",
   "metadata": {},
   "source": [
    "# Cleaning\n",
    "Standard tweet cleaning.  Cleaning found at:\n",
    "https://stackoverflow.com/questions/64719706/cleaning-twitter-data-pandas-python\n",
    "\n",
    "# Tokenizatin\n",
    "I tolkenized the tweets in preparation to convert to tensors for embedding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f63f3726-3a79-4c8f-97bf-2a1a3f6cca61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thank you Chatgpt for this\n",
    "def download_file_from_github(url):\n",
    "    \"\"\"Download a file from a GitHub URL and return its contents as a list of lines.\"\"\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        lines = response.text.splitlines()  # Split the content into lines\n",
    "        return lines  # You could change this to `set(lines)` if you need a set instead of a list\n",
    "    else:\n",
    "        raise Exception(f\"Failed to download file: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1b87ba1-c57f-4159-8c9f-e60f9c8f5365",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vocab = nltk.lm.Vocabulary()\n",
    "output = pd.DataFrame()\n",
    "stop_words = download_file_from_github(\"https://raw.githubusercontent.com/stopwords-iso/stopwords-en/master/raw/stop-words-english1.txt\")\n",
    "contractions = download_file_from_github(\"https://gist.githubusercontent.com/J3RN/ed7b420a6ea1d5bd6d06/raw/acda66b325a2b4d7282fb602a7551912cdc81e74/contractions.txt\")\n",
    "def cleaning(line):\n",
    "    tweet = line['text']\n",
    "    tweet = tweet.lower()\n",
    "    tweet = re.sub(\"@[A-Za-z0-9]+\",\"\",tweet) #Remove @ sign\n",
    "    tweet = re.sub(r\"(?:\\@|http?\\://|https?\\://|www)\\S+\", \"\", tweet) #Remove http links\n",
    "    tweet = \" \".join(tweet.split())\n",
    "    tweet = emoji.replace_emoji(tweet, '') #Remove Emojis\n",
    "    tweet = tweet.replace(\"#\", \"\").replace(\"_\", \" \") #Remove hashtag sign but keep the text\n",
    "    tweet = re.sub(r'[^a-z]', ' ', tweet) # Strip all symbols and replace with +\n",
    "    tweet = re.sub(r'\\b\\w{1,2}\\b+', '', tweet) #get rid of all words <= 2 characters\n",
    "    #Tolkenize the Text\n",
    "    word_tokens = word_tokenize(tweet)\n",
    "    word_tokens = [w for w in word_tokens if not w in stop_words]\n",
    "    word_tokens = [w for w in word_tokens if not w in contractions]\n",
    "    #tknzr = nltk.tokenize.casual.TweetTokenizer(preserve_case = False, strip_handles = True, reduce_len = True)\n",
    "    #tweet = tknzr.tokenize(tweet)\n",
    "    vocab.update(word_tokens)\n",
    "    return word_tokens \n",
    "train['cleaned_text'] = train.apply(cleaning, axis = 1)\n",
    "train['text'] = train['cleaned_text']\n",
    "train = train.drop(labels = ['cleaned_text'], axis = 1)\n",
    "test['cleaned_text'] = test.apply(cleaning, axis = 1)\n",
    "test['text'] = test['cleaned_text']\n",
    "test = test.drop(labels = ['cleaned_text'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "545afe54-0a63-4077-b8d1-378bccc2abee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 text  target\n",
      "0         [deeds, reason, earthquake, allah, forgive]       1\n",
      "1                 [forest, fire, ronge, sask, canada]       1\n",
      "2   [residents, asked, shelter, place, notified, o...       1\n",
      "3   [people, receive, wildfires, evacuation, order...       1\n",
      "4   [photo, ruby, alaska, smoke, wildfires, pours,...       1\n",
      "5   [rockyfire, update, california, hwy, closed, d...       1\n",
      "6   [flood, disaster, heavy, rain, flash, flooding...       1\n",
      "7                            [top, hill, fire, woods]       1\n",
      "8   [emergency, evacuation, happening, building, s...       1\n",
      "9                     [afraid, tornado, coming, area]       1\n",
      "10                         [people, died, heat, wave]       1\n",
      "11  [haha, south, tampa, flooded, hah, wait, live,...       1\n",
      "12  [raining, flooding, florida, tampabay, tampa, ...       1\n",
      "13              [flood, bago, myanmar, arrived, bago]       1\n",
      "14  [damage, school, bus, multi, car, crash, break...       1\n",
      "15                                              [man]       0\n",
      "16                                     [love, fruits]       0\n",
      "17                                   [summer, lovely]       0\n",
      "18                                        [car, fast]       0\n",
      "19                                  [goooooooaaaaaal]       0\n",
      "Rows processed with 0 remaining words\n",
      "     text  target\n",
      "30     []       0\n",
      "428    []       0\n",
      "1664   []       0\n",
      "3584   []       0\n",
      "3680   []       0\n",
      "4297   []       0\n",
      "4891   []       0\n",
      "5564   []       0\n"
     ]
    }
   ],
   "source": [
    "print(train.head(20))\n",
    "zero_length_lists = train[train['text'].apply(lambda x: len(x) == 0)]\n",
    "print(\"Rows processed with 0 remaining words\")\n",
    "print(zero_length_lists)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1861cc4e-7347-46cd-ae06-45f49111876b",
   "metadata": {},
   "source": [
    "# Additional EDA.  \n",
    "Thats intresting.  All the items that are all stopwords are going to be no an emergency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09373cd4-f572-4d0d-83c4-db045188269e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[train['text'].apply(lambda x: len(x) != 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38e526e0-5b4a-44ab-93bd-d350e646191b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets\n",
    "train_dat = train.sample(frac = .9)\n",
    "test_dat = train.drop(train_dat.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45c83554-d800-4f68-9d79-30a1c5cd9658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16548\n"
     ]
    }
   ],
   "source": [
    "vocab_dict = {}\n",
    "for item in list(vocab):\n",
    "    vocab_dict[item] = vocab[item]\n",
    "print(len(vocab_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17c0649-e1af-40ff-8d42-3ab6b7b138af",
   "metadata": {},
   "source": [
    "# Vectorizing\n",
    "I used pytorchs internal vectorizer to vectorize the text.  I\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "227296e5-b28c-4a61-81c5-d47a90151761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Importing from https://pytorch.org/tutorials/beginner/basics/data_tutorial.html?highlight=dataloader\n",
    "\n",
    "#tokenize the text for tensor load\n",
    "def tokenize(text):\n",
    "    # Your tokenizer logic here\n",
    "    return [vocab[token] for token in text.split()]\n",
    "\n",
    "class CustomTextDataset(Dataset):\n",
    "    def __init__(self, data,vocab, transform=None, target_transform=None):\n",
    "        self.data = data\n",
    "        self.vocab = vocab\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tweet = self.data.iloc[idx, 0]\n",
    "        label = self.data.iloc[idx, 1]\n",
    "        tokenized_tweet = [self.vocab.get(token, 0) for token in tweet]  # Tokenize and handle unknown tokens\n",
    "        tweet_tensor  = torch.tensor(tokenized_tweet, dtype=torch.long)\n",
    "        label_tensor = torch.tensor(label, dtype=torch.float)\n",
    "        \n",
    "        if self.transform:\n",
    "            tweet_tensor = self.transform(tweet_tensor)\n",
    "        if self.target_transform:\n",
    "            label_tensor = self.target_transform(label_tensor)\n",
    "\n",
    "        return tweet_tensor, label_tensor\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list = [], []\n",
    "    for (_text, _label) in batch:\n",
    "        label_list.append(_label)\n",
    "        text_list.append(_text.clone().detach())\n",
    "    return pad_sequence(text_list, batch_first=True), torch.tensor(label_list, dtype=torch.float)\n",
    "\n",
    "# Creating training and validation datasets\n",
    "training_set = CustomTextDataset(train_dat, vocab_dict)\n",
    "validation_set = CustomTextDataset(test_dat, vocab_dict)\n",
    "#test set = CustomTextDataset(val_dat, vocab)\n",
    "\n",
    "# Creating data loaders\n",
    "training_loader = torch.utils.data.DataLoader(training_set, batch_size=32, shuffle=True, collate_fn=collate_batch)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=32, shuffle=False, collate_fn=collate_batch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2b0482-3bc0-4937-bc04-21171bef4d13",
   "metadata": {},
   "source": [
    "# Model approach\n",
    "I'm choosing to use a RNN were I only select the last value as the output, so many in and 1 out.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9a443764-d66c-45fa-8d15-d3cb05df622c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial Setup\n",
    "best_model_params = []\n",
    "class Tweets_RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers, nonlinearity):\n",
    "        super(Tweets_RNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn_layer = nn.RNN(input_size=embedding_dim, hidden_size=hidden_size, num_layers=num_layers,\n",
    "                                nonlinearity=nonlinearity, bias=True, bidirectional=True)\n",
    "        # Add TanH Layer\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=.75)\n",
    "        #self.relu = nn.ReLU()\n",
    "        self.linear = nn.Linear(hidden_size * 2, 1)\n",
    "        # Step Down linear layers?\n",
    "        #self.output_layer = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_text):\n",
    "        # Assuming input_text is already a LongTensor that has been prepared outside the model\n",
    "        embedded = self.embedding(input_text)\n",
    "        rnn_output, _ = self.rnn_layer(embedded)\n",
    "        dropout = self.dropout(rnn_output)\n",
    "        output = dropout[:, :, :]\n",
    "        linear = self.linear(output)\n",
    "        final_output = linear.mean(dim=1)\n",
    "        #print(final_output.shape)\n",
    "        return final_output.squeeze()  # Squeeze to remove the extra dimension\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5090bbb-2db1-4606-bec5-dc878ab4a289",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ca084162-e5c6-4c64-aaf9-ffafc0ccc723",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_one_epoch(epoch_index, model,training_loader, loss_fn, optimizer, tb_writer):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, data in enumerate(training_loader):\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, labels = data\n",
    "        inputs_cuda = inputs.to('cuda')\n",
    "        labels_cuda = labels.to('cuda')\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs = model(inputs_cuda)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_fn(outputs, labels_cuda)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 9:\n",
    "            last_loss = running_loss / 1000 # loss per batch\n",
    "            #print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            tb_x = epoch_index * len(training_loader) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0\n",
    "\n",
    "    return last_loss\n",
    "\n",
    "# test execution\n",
    "#model = Tweets(vocab_size=30000, embedding_dim =1024).to('cuda')\n",
    "#loss_fn = nn.BCEWithLogitsLoss()\n",
    "#optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "#tb_writer = SummaryWriter('logs')\n",
    "#train_one_epoch(1,model, training_loader, loss_fn, optimizer, tb_writer)\n",
    "#tb_writer.close()\n",
    "\n",
    "output_data = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e25815-0179-4867-bf2d-87317bd2bbce",
   "metadata": {},
   "source": [
    "Ok, .68 validation loss.  Thats not good.  I'm going to run a optomizer to try and find the optimal variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a65f7ffd-cba2-453d-a940-670aac21f118",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_global_vloss = 1\n",
    "\n",
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "def run_model(output_data, model, epochs = 20):\n",
    "    \n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "    \n",
    "    timestamp = datetime.now(timezone.utc).strftime('%Y%m%d_%H%M%S')\n",
    "    writer = SummaryWriter('runs/fashion_trainer_{}'.format(timestamp))\n",
    "    \n",
    "    epoch_number = 0\n",
    "    \n",
    "    best_vloss = 1.\n",
    "    best_vacc = 0\n",
    "    best_vacc_count = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print('EPOCH {}:'.format(epoch_number + 1))\n",
    "    \n",
    "        # Make sure gradient tracking is on, and do a pass over the data\n",
    "        model.train(True)\n",
    "        avg_loss = train_one_epoch(epoch_number,model, training_loader, loss_fn, optimizer, writer)\n",
    "        \n",
    "    \n",
    "        running_vloss = 0.0\n",
    "        running_vacc = 0.0\n",
    "        # Set the model to evaluation mode, disabling dropout and using population\n",
    "        # statistics for batch normalization.\n",
    "        model.eval()\n",
    "        # Disable gradient computation and reduce memory consumption.\n",
    "        with torch.no_grad():\n",
    "            for i, vdata in enumerate(validation_loader):\n",
    "                vinputs, vlabels = vdata\n",
    "                vinputs_cuda = vinputs.to('cuda')\n",
    "                vlabels_cuda = vlabels.to('cuda')\n",
    "                voutputs = model(vinputs_cuda)\n",
    "                vloss = loss_fn(voutputs, vlabels_cuda)\n",
    "                vacc = binary_accuracy(voutputs, vlabels_cuda)\n",
    "                running_vloss += vloss\n",
    "                running_vacc += vacc.item()\n",
    "                \n",
    "        avg_vloss = running_vloss / (i + 1)\n",
    "        avg_vacc = running_vacc / (i + 1)\n",
    "        print('LOSS train {} valid {} Binary Acc {}'.format(avg_loss, avg_vloss, avg_vacc))\n",
    "        \n",
    "        # Log the running loss averaged per batch\n",
    "        # for both training and validation\n",
    "        writer.add_scalars('Training vs. Validation Loss',\n",
    "                        { 'Training' : avg_loss, 'Validation' : avg_vloss},\n",
    "                        epoch_number + 1)\n",
    "        writer.flush()\n",
    "        new_row = {\n",
    "            \"run\": run_num,\n",
    "            \"train_loss\": str(avg_loss),\n",
    "            \"val_loss\": str(avg_vloss),\n",
    "            \"val_acc\": avg_vacc\n",
    "        }\n",
    "        output_data = pd.concat([output_data, pd.Series(new_row)], ignore_index=True)\n",
    "        # Track best performance, and save the model's state\n",
    "        global best_global_vloss\n",
    "        global best_model_params\n",
    "        if avg_vloss < best_vloss:\n",
    "            best_vloss = avg_vloss\n",
    "        if avg_vloss < best_global_vloss:\n",
    "            model_path = 'best_model.model'\n",
    "            best_global_vloss = avg_vloss\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "        #Calculate Best VAccuracy\n",
    "        if best_vacc < avg_vacc:\n",
    "            best_vacc = avg_vacc\n",
    "            best_vacc_count = 0\n",
    "        else: \n",
    "            best_vacc_count += 1\n",
    "\n",
    "        #Cutoff the model if the VACC goest down after 2 cycles\n",
    "        #if best_vacc_count == 3:\n",
    "            #break\n",
    "        \n",
    "        epoch_number += 1\n",
    "    print(f'Best Validation Loss {best_vloss}  Best Validation Accuracy {best_vacc}')\n",
    "    torch.cuda.empty_cache()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8aab295-5a3b-41e9-bf5f-ff9ad7ce896d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "LOSS train 0.006784742295742035 valid 0.6564794778823853 Binary Acc 0.6143229169150194\n",
      "EPOCH 2:\n",
      "LOSS train 0.006382430672645569 valid 0.6174231767654419 Binary Acc 0.618593749900659\n",
      "EPOCH 3:\n",
      "LOSS train 0.0060724313855171205 valid 0.5975783467292786 Binary Acc 0.6764062494039536\n",
      "EPOCH 4:\n",
      "LOSS train 0.005368231505155563 valid 0.585707426071167 Binary Acc 0.6563541665673256\n",
      "EPOCH 5:\n",
      "LOSS train 0.005397511601448059 valid 0.563495397567749 Binary Acc 0.6753124992052714\n",
      "EPOCH 6:\n",
      "LOSS train 0.005509853363037109 valid 0.5580012798309326 Binary Acc 0.7085937509934107\n",
      "EPOCH 7:\n",
      "LOSS train 0.0053938995599746705 valid 0.5517502427101135 Binary Acc 0.7072916676600774\n",
      "EPOCH 8:\n",
      "LOSS train 0.00611026644706726 valid 0.5468348264694214 Binary Acc 0.7111979176600774\n",
      "EPOCH 9:\n",
      "LOSS train 0.005376096844673156 valid 0.5433369874954224 Binary Acc 0.7020833343267441\n",
      "EPOCH 10:\n",
      "LOSS train 0.005261266320943833 valid 0.5419866442680359 Binary Acc 0.7085937509934107\n",
      "EPOCH 11:\n",
      "LOSS train 0.0055322741866111755 valid 0.5422835350036621 Binary Acc 0.7033854176600774\n",
      "EPOCH 12:\n",
      "LOSS train 0.004962581932544708 valid 0.5465574860572815 Binary Acc 0.7004166667660078\n",
      "EPOCH 13:\n",
      "LOSS train 0.005457256048917771 valid 0.5376337766647339 Binary Acc 0.7229166676600774\n",
      "EPOCH 14:\n",
      "LOSS train 0.00504495957493782 valid 0.530366063117981 Binary Acc 0.7190104176600774\n",
      "EPOCH 15:\n",
      "LOSS train 0.005070646405220032 valid 0.5319122076034546 Binary Acc 0.7190104176600774\n",
      "EPOCH 16:\n",
      "LOSS train 0.005316927820444107 valid 0.5308983325958252 Binary Acc 0.7216145843267441\n",
      "EPOCH 17:\n",
      "LOSS train 0.00499365359544754 valid 0.527717649936676 Binary Acc 0.7190104176600774\n",
      "EPOCH 18:\n",
      "LOSS train 0.005069327652454376 valid 0.5282787084579468 Binary Acc 0.7177083343267441\n",
      "EPOCH 19:\n",
      "LOSS train 0.005821454763412476 valid 0.5298143625259399 Binary Acc 0.7164062509934107\n",
      "EPOCH 20:\n",
      "LOSS train 0.005362658023834228 valid 0.5269128680229187 Binary Acc 0.7242187509934107\n",
      "EPOCH 21:\n",
      "LOSS train 0.005106665670871735 valid 0.5292037725448608 Binary Acc 0.7177083343267441\n",
      "EPOCH 22:\n",
      "LOSS train 0.005190953880548477 valid 0.5326895713806152 Binary Acc 0.7164062509934107\n",
      "EPOCH 23:\n",
      "LOSS train 0.0050682657063007355 valid 0.5301542282104492 Binary Acc 0.7125000009934107\n",
      "EPOCH 24:\n",
      "LOSS train 0.005151796251535416 valid 0.5288994908332825 Binary Acc 0.7180729160706202\n",
      "EPOCH 25:\n",
      "LOSS train 0.005138182252645493 valid 0.5266661643981934 Binary Acc 0.7216145843267441\n",
      "EPOCH 26:\n"
     ]
    }
   ],
   "source": [
    "\n",
    "run_num = 1\n",
    "embedding_dim = 64\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "nonlinearity = 'relu'\n",
    "\n",
    "model = Tweets_RNN(vocab_size=len(vocab_dict), embedding_dim = embedding_dim,\n",
    "                   hidden_size = hidden_size, num_layers= num_layers,\n",
    "                   nonlinearity = nonlinearity\n",
    "                  ).to('cuda')\n",
    "\n",
    "run = run_model(  output_data = output_data,\n",
    "                  epochs = 40, model = model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acc7dec-166b-4e58-b538-b025a0e02d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tweets_LSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, bias, num_layers, dropout):\n",
    "        super(Tweets_LSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm_layer = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_size, \n",
    "                                  num_layers=num_layers,bias = True,\n",
    "                                  bidirectional=True, dropout = dropout)\n",
    "        # Add TanH Layer\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=.75)\n",
    "        #self.relu = nn.ReLU()\n",
    "        self.linear = nn.Linear(hidden_size * 2, 1)\n",
    "        # Step Down linear layers?\n",
    "        #self.output_layer = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_text):\n",
    "        # Assuming input_text is already a LongTensor that has been prepared outside the model\n",
    "        embedded = self.embedding(input_text)\n",
    "        lstm_output, _ = self.lstm_layer(embedded)\n",
    "        dropout = self.dropout(lstm_output)\n",
    "        output = dropout[:, :, :]\n",
    "        linear = self.linear(output)\n",
    "        final_output = linear.mean(dim=1)\n",
    "        #print(final_output.shape)\n",
    "        return final_output.squeeze()  # Squeeze to remove the extra dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7cdc04e6-f105-40ae-b45e-0427741369c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "LOSS train 0.006653462767601013 valid 0.6654738783836365 Binary Acc 0.5691145832339922\n",
      "EPOCH 2:\n",
      "LOSS train 0.006608706593513489 valid 0.6449795961380005 Binary Acc 0.595156249900659\n",
      "EPOCH 3:\n",
      "LOSS train 0.006077652394771576 valid 0.6134889125823975 Binary Acc 0.6280729162196318\n",
      "EPOCH 4:\n",
      "LOSS train 0.005860358119010926 valid 0.5942034721374512 Binary Acc 0.650937500099341\n",
      "EPOCH 5:\n",
      "LOSS train 0.005917282462120056 valid 0.5878579020500183 Binary Acc 0.6868229160706202\n",
      "EPOCH 6:\n",
      "LOSS train 0.006153228044509887 valid 0.5774403810501099 Binary Acc 0.6656250009934107\n",
      "EPOCH 7:\n",
      "LOSS train 0.005667667090892792 valid 0.5714709758758545 Binary Acc 0.6734375009934107\n",
      "EPOCH 8:\n",
      "LOSS train 0.005991019546985626 valid 0.5699287056922913 Binary Acc 0.6838541676600774\n",
      "EPOCH 9:\n",
      "LOSS train 0.005684522300958633 valid 0.5655550956726074 Binary Acc 0.6812500009934107\n",
      "EPOCH 10:\n",
      "LOSS train 0.005284210145473481 valid 0.5634011030197144 Binary Acc 0.6851562509934107\n",
      "EPOCH 11:\n",
      "LOSS train 0.005533837646245957 valid 0.5573535561561584 Binary Acc 0.6929687509934107\n",
      "EPOCH 12:\n",
      "LOSS train 0.0056976030468940736 valid 0.5579087734222412 Binary Acc 0.6942708343267441\n",
      "EPOCH 13:\n",
      "LOSS train 0.00566553995013237 valid 0.5546383857727051 Binary Acc 0.7007812509934107\n",
      "EPOCH 14:\n",
      "LOSS train 0.0057017748057842255 valid 0.5491907596588135 Binary Acc 0.7125000009934107\n",
      "EPOCH 15:\n",
      "LOSS train 0.0054291199743747715 valid 0.5488688945770264 Binary Acc 0.7059895843267441\n",
      "EPOCH 16:\n",
      "LOSS train 0.005765705972909927 valid 0.5438808798789978 Binary Acc 0.7190104176600774\n",
      "EPOCH 17:\n",
      "LOSS train 0.005568310737609863 valid 0.5385728478431702 Binary Acc 0.7203125009934107\n",
      "EPOCH 18:\n",
      "LOSS train 0.005592084854841232 valid 0.5410556793212891 Binary Acc 0.7190104176600774\n",
      "EPOCH 19:\n",
      "LOSS train 0.0052228236794471744 valid 0.5368955135345459 Binary Acc 0.7151041676600774\n",
      "EPOCH 20:\n",
      "LOSS train 0.005195375919342041 valid 0.5389828681945801 Binary Acc 0.7151041676600774\n",
      "EPOCH 21:\n",
      "LOSS train 0.005726812154054642 valid 0.5366302132606506 Binary Acc 0.7190104176600774\n",
      "EPOCH 22:\n",
      "LOSS train 0.005548505306243896 valid 0.5334399938583374 Binary Acc 0.7190104176600774\n",
      "EPOCH 23:\n",
      "LOSS train 0.005590094119310379 valid 0.5306435823440552 Binary Acc 0.7203125009934107\n",
      "EPOCH 24:\n",
      "LOSS train 0.005325670063495636 valid 0.5340460538864136 Binary Acc 0.7085937509934107\n",
      "EPOCH 25:\n",
      "LOSS train 0.005445187360048294 valid 0.529863715171814 Binary Acc 0.7151041676600774\n",
      "EPOCH 26:\n",
      "LOSS train 0.005471548676490784 valid 0.5285318493843079 Binary Acc 0.7242187509934107\n",
      "EPOCH 27:\n",
      "LOSS train 0.005316440880298615 valid 0.5240395069122314 Binary Acc 0.7216145843267441\n",
      "EPOCH 28:\n",
      "LOSS train 0.005547100484371186 valid 0.5266170501708984 Binary Acc 0.7284895827372869\n",
      "EPOCH 29:\n",
      "LOSS train 0.004719101130962372 valid 0.5256047248840332 Binary Acc 0.7268229176600774\n",
      "EPOCH 30:\n",
      "LOSS train 0.005253564834594726 valid 0.5293456315994263 Binary Acc 0.7203125009934107\n",
      "Best Validation Loss 0.5240395069122314  Best Validation Accuracy 0.7284895827372869\n"
     ]
    }
   ],
   "source": [
    "\n",
    "run_num = 2\n",
    "embedding_dim = 64\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "\n",
    "model = Tweets_LSTM(vocab_size=len(vocab_dict), \n",
    "                    embedding_dim = embedding_dim,\n",
    "                    hidden_size = hidden_size, \n",
    "                    num_layers= num_layers, \n",
    "                    dropout = 0.5, bias = True\n",
    "                  ).to('cuda')\n",
    "\n",
    "run = run_model(  output_data = output_data,\n",
    "                  epochs = 40, model = model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "411c7748-bee2-4f12-828e-9cebe830725e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tweets_GRU(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, bias, num_layers, dropout):\n",
    "        super(Tweets_GRU, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.GRU_layer = nn.GRU(input_size=embedding_dim, hidden_size=hidden_size, \n",
    "                                  num_layers=num_layers,bias = True,\n",
    "                                  bidirectional=True, dropout = dropout)\n",
    "        # Add TanH Layer\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=.75)\n",
    "        #self.relu = nn.ReLU()\n",
    "        self.linear = nn.Linear(hidden_size * 2, 1)\n",
    "        # Step Down linear layers?\n",
    "        #self.output_layer = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_text):\n",
    "        # Assuming input_text is already a LongTensor that has been prepared outside the model\n",
    "        embedded = self.embedding(input_text)\n",
    "        gru_output, _ = self.GRU_layer(embedded)\n",
    "        dropout = self.dropout(gru_output)\n",
    "        output = dropout[:, :, :]\n",
    "        linear = self.linear(output)\n",
    "        final_output = linear.mean(dim=1)\n",
    "        #print(final_output.shape)\n",
    "        return final_output.squeeze()  # Squeeze to remove the extra dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3cd7feb8-64ad-48ef-8aaa-e494dd6ec7c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "LOSS train 0.00673525458574295 valid 0.6602927446365356 Binary Acc 0.5691145832339922\n",
      "EPOCH 2:\n",
      "LOSS train 0.006640859603881836 valid 0.6548405885696411 Binary Acc 0.602968749900659\n",
      "EPOCH 3:\n",
      "LOSS train 0.006240485608577729 valid 0.632247805595398 Binary Acc 0.602968749900659\n",
      "EPOCH 4:\n",
      "LOSS train 0.006185948431491852 valid 0.6161451935768127 Binary Acc 0.6405208334326744\n",
      "EPOCH 5:\n",
      "LOSS train 0.006211955487728119 valid 0.6016430854797363 Binary Acc 0.654843750099341\n",
      "EPOCH 6:\n",
      "LOSS train 0.006010456323623657 valid 0.5885050892829895 Binary Acc 0.670468750099341\n",
      "EPOCH 7:\n",
      "LOSS train 0.005710723817348481 valid 0.5827634334564209 Binary Acc 0.6773437509934107\n",
      "EPOCH 8:\n",
      "LOSS train 0.0059095139503479 valid 0.584218442440033 Binary Acc 0.6916666676600774\n",
      "EPOCH 9:\n",
      "LOSS train 0.006189924955368042 valid 0.5701404809951782 Binary Acc 0.6734375009934107\n",
      "EPOCH 10:\n",
      "LOSS train 0.005406829923391342 valid 0.5701776742935181 Binary Acc 0.6760416676600774\n",
      "EPOCH 11:\n",
      "LOSS train 0.005909460335969925 valid 0.5669885873794556 Binary Acc 0.7072916676600774\n",
      "EPOCH 12:\n",
      "LOSS train 0.005649236738681793 valid 0.5641103982925415 Binary Acc 0.6981770843267441\n",
      "EPOCH 13:\n",
      "LOSS train 0.00555042839050293 valid 0.5770872235298157 Binary Acc 0.6795833334326744\n",
      "EPOCH 14:\n",
      "LOSS train 0.005658397853374481 valid 0.5586682558059692 Binary Acc 0.7033854176600774\n",
      "EPOCH 15:\n",
      "LOSS train 0.005586790084838867 valid 0.557385265827179 Binary Acc 0.7072916676600774\n",
      "EPOCH 16:\n",
      "LOSS train 0.005590346097946167 valid 0.5537416338920593 Binary Acc 0.6994791676600774\n",
      "EPOCH 17:\n",
      "LOSS train 0.005499603986740112 valid 0.5531244277954102 Binary Acc 0.7046875009934107\n",
      "EPOCH 18:\n",
      "LOSS train 0.005319419682025909 valid 0.5560288429260254 Binary Acc 0.7059895843267441\n",
      "EPOCH 19:\n",
      "LOSS train 0.005877549171447754 valid 0.5561689734458923 Binary Acc 0.7059895843267441\n",
      "EPOCH 20:\n",
      "LOSS train 0.005387345045804978 valid 0.5582977533340454 Binary Acc 0.6955729176600774\n",
      "EPOCH 21:\n",
      "LOSS train 0.005260142773389816 valid 0.5499915480613708 Binary Acc 0.7020833343267441\n",
      "EPOCH 22:\n",
      "LOSS train 0.005920783549547196 valid 0.5530190467834473 Binary Acc 0.7111979176600774\n",
      "EPOCH 23:\n",
      "LOSS train 0.005818660199642181 valid 0.5503905415534973 Binary Acc 0.6994791676600774\n",
      "EPOCH 24:\n",
      "LOSS train 0.0054144606590271 valid 0.5437005758285522 Binary Acc 0.7085937509934107\n",
      "EPOCH 25:\n",
      "LOSS train 0.00517319792509079 valid 0.5518478155136108 Binary Acc 0.6994791676600774\n",
      "EPOCH 26:\n",
      "LOSS train 0.005466347545385361 valid 0.5418552160263062 Binary Acc 0.7059895843267441\n",
      "EPOCH 27:\n",
      "LOSS train 0.0056979919075965885 valid 0.5439062118530273 Binary Acc 0.7059895843267441\n",
      "EPOCH 28:\n",
      "LOSS train 0.005222826898097992 valid 0.5390597581863403 Binary Acc 0.7033854176600774\n",
      "EPOCH 29:\n",
      "LOSS train 0.005610901027917862 valid 0.5394576787948608 Binary Acc 0.7085937509934107\n",
      "EPOCH 30:\n",
      "LOSS train 0.005613810241222382 valid 0.5460106134414673 Binary Acc 0.7098958343267441\n",
      "Best Validation Loss 0.5390597581863403  Best Validation Accuracy 0.7111979176600774\n"
     ]
    }
   ],
   "source": [
    "run_num = 3\n",
    "#embedding_dim = 32\n",
    "#hidden_size = 32\n",
    "#num_layers = 1\n",
    "\n",
    "model = Tweets_GRU(vocab_size=len(vocab_dict), \n",
    "                    embedding_dim = embedding_dim,\n",
    "                    hidden_size = hidden_size, \n",
    "                    num_layers= num_layers, \n",
    "                    dropout = 0.5, bias = True\n",
    "                  ).to('cuda')\n",
    "\n",
    "run = run_model(  output_data = output_data,\n",
    "                  epochs = 40, model = model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "84dd9c1a-852a-48fb-ac73-f92cbe6b65ab",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Tweets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[80], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m test \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(test_path, header \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mTweets\u001b[49m(vocab_size \u001b[38;5;241m=\u001b[39m best_model_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvocab_size\u001b[39m\u001b[38;5;124m\"\u001b[39m], \n\u001b[0;32m      3\u001b[0m                                  embedding_dim \u001b[38;5;241m=\u001b[39m best_model_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding_dim\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m      4\u001b[0m                                  hidden_size \u001b[38;5;241m=\u001b[39m best_model_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhidden_size\u001b[39m\u001b[38;5;124m\"\u001b[39m], \n\u001b[0;32m      5\u001b[0m                                  num_layers \u001b[38;5;241m=\u001b[39m best_model_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_layers\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m      6\u001b[0m                                  nonlinearity \u001b[38;5;241m=\u001b[39m best_model_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonlinearity\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m      7\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_model.model\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39meval()  \u001b[38;5;66;03m# Set the model to evaluation mode\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Tweets' is not defined"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv(test_path, header = 0)\n",
    "model = Tweets(vocab_size = best_model_params[\"vocab_size\"], \n",
    "                                 embedding_dim = best_model_params[\"embedding_dim\"],\n",
    "                                 hidden_size = best_model_params[\"hidden_size\"], \n",
    "                                 num_layers = best_model_params[\"num_layers\"],\n",
    "                                 nonlinearity = best_model_params[\"nonlinearity\"])\n",
    "model.load_state_dict(torch.load('best_model.model'))\n",
    "model.eval()  # Set the model to evaluation mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d79123-ee72-4841-aa16-3c27e33cb684",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
